{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VPG.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOR0JBXfRy07ahPNzT0WW6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AGKhalil/RL_implements/blob/master/VPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFvoUhM_ECGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install numpy tqdm gym matplotlib argparse torch wandb scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkSI7_AFjBkK",
        "colab_type": "text"
      },
      "source": [
        "## Restart Runtime\n",
        "This is done to ensure the installed dependencies and game are loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-lOzFLIetQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "  \n",
        "restart_runtime()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2L7RqC354c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import gym\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.tensor as Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "import wandb\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZtzTt-7M0xg",
        "colab_type": "code",
        "outputId": "970e5d1c-15d5-4b19-abf2-effb420a46a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# WandB â€“ Login to your wandb account so you can log all your metrics\n",
        "!wandb login"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: fd1a686e3fb538374e472fc536037d249adef19f\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9BCX4Bj54c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, env.action_space.n)\n",
        "            \n",
        "    def forward(self, x, softmax=False):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6zEGye-54dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action(obs):\n",
        "    current_policy = get_current_policy(obs)\n",
        "    probs = F.softmax(current_policy)\n",
        "    dist = torch.distributions.Categorical(probs)\n",
        "    act = dist.sample().item()\n",
        "    return act, F.log_softmax(current_policy).squeeze(0)[act]\n",
        "\n",
        "def get_current_policy(obs):\n",
        "    return policy.forward(torch.from_numpy(obs).float().unsqueeze(0).to(gpu))\n",
        "\n",
        "def reward_to_go(a):\n",
        "    return np.sum([a[i] * np.power(gamma, i) for i in range(len(a))]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfUTaY0rObsg",
        "colab_type": "code",
        "outputId": "acf006dd-d99f-48b0-c77f-2c26ec3206c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKpFBYW154c-",
        "colab_type": "code",
        "outputId": "fbc6fc6b-83c7-4df5-8c3b-e2144cbf7178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "wandb.init(entity=\"agkhalil\", project=\"pytorch-vpg-cartpole\")\n",
        "wandb.watch_called = False\n",
        "\n",
        "config = wandb.config\n",
        "config.batch_size = 50\n",
        "config.episodes = 2000\n",
        "config.lr = 0.0005\n",
        "config.seed = 42\n",
        "config.gamma = 0.99\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "gpu = torch.device('cuda:0')\n",
        "torch.manual_seed(config.seed)\n",
        "learning_rate = config.lr\n",
        "batch_size = config.batch_size\n",
        "policy = MLP().to(gpu)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "EPISODES = config.episodes\n",
        "gamma = config.gamma\n",
        "\n",
        "wandb.watch(policy, log=\"all\")\n",
        "\n",
        "for episode in tqdm(range(0, EPISODES)):\n",
        "    old_rewards = []\n",
        "    rewards = []\n",
        "    log_soft = []\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    step = 0\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        action, log_prob = get_action(obs)\n",
        "        new_obs, rew, done, _ = env.step(action)\n",
        "        reward += rew * np.power(gamma, step)\n",
        "        old_rewards.append(reward)        \n",
        "        rewards.append(rew)\n",
        "        log_soft.append(log_prob)\n",
        "        step += 1\n",
        "        obs = new_obs\n",
        "\n",
        "    discounted_rewards = [reward_to_go(rewards[i:]) for i in range(len(rewards))]\n",
        "    optimizer.zero_grad()\n",
        "    discounted_rewards = torch.tensor(discounted_rewards).to(gpu)\n",
        "    advantage = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + eps)\n",
        "    loss = [-advantage[i] * log_soft[i] for i in range(len(advantage))]\n",
        "    loss = torch.stack(loss)\n",
        "    loss.to(gpu)\n",
        "    loss.sum().backward()\n",
        "    optimizer.step()\n",
        "    wandb.log({\n",
        "        \"Episode reward\": step,\n",
        "        \"Loss\": loss.cpu(),\n",
        "        }, step=episode)\n",
        "\n",
        "torch.save(policy.state_dict(), \"model.h5\")\n",
        "wandb.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/agkhalil/pytorch-vpg-cartpole\" target=\"_blank\">https://app.wandb.ai/agkhalil/pytorch-vpg-cartpole</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/agkhalil/pytorch-vpg-cartpole/runs/suet6aum\" target=\"_blank\">https://app.wandb.ai/agkhalil/pytorch-vpg-cartpole/runs/suet6aum</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1326/2000 [03:34<02:27,  4.58it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU7Fd_Tjp_Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_rewards = torch.tensor(old_rewards)\n",
        "old_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diFhYKmLtUNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discounted_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4wwTJJN54d5",
        "colab_type": "code",
        "outputId": "ef11c6c6-d043-4e41-eed9-04c729a2f3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tot_per = []\n",
        "epsilon = 0\n",
        "\n",
        "for ep in tqdm(range(0, 100)):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    tot_rew = 0\n",
        "    while not done:\n",
        "        act = get_action(obs)\n",
        "        obs, rew, done, _ = env.step(act)\n",
        "        tot_rew += rew\n",
        "#         env.render()\n",
        "    tot_per.append(tot_rew)\n",
        "np.mean(tot_per)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:06<00:00, 13.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199.11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKuVvjt7Mn_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}