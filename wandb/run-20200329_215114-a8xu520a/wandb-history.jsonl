{"Episode reward": -32.89911162102427, "Episode length": 999, "Policy Loss": -0.024365108460187912, "Value Loss": 0.08048012107610703, "_runtime": 9615.139661550522, "_timestamp": 1585518693.5603912, "_step": 0}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": -2.8642306327819824, "Value Loss": 54.526493072509766, "_runtime": 9616.656688690186, "_timestamp": 1585518695.0774183, "_step": 1}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 0.7978102564811707, "Value Loss": 486.8016052246094, "_runtime": 9618.289554357529, "_timestamp": 1585518696.710284, "_step": 2}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": -6.597561359405518, "Value Loss": 969.739501953125, "_runtime": 9619.841539621353, "_timestamp": 1585518698.2622693, "_step": 3}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": -149.8247528076172, "Value Loss": 2827.54736328125, "_runtime": 9621.3908264637, "_timestamp": 1585518699.811556, "_step": 4}
{"Episode reward": -93.49550017429114, "Episode length": 999, "Policy Loss": -3.018832206726074, "Value Loss": 40397.76953125, "_runtime": 9622.99238729477, "_timestamp": 1585518701.413117, "_step": 5}
{"Episode reward": -88.84512237777584, "Episode length": 999, "Policy Loss": -4.173803329467773, "Value Loss": 674.6838989257812, "_runtime": 9624.566325426102, "_timestamp": 1585518702.987055, "_step": 6}
{"Episode reward": -98.58662203120396, "Episode length": 999, "Policy Loss": -10.92078971862793, "Value Loss": 94.63722229003906, "_runtime": 9626.115711450577, "_timestamp": 1585518704.536441, "_step": 7}
{"Episode reward": -91.6871680682174, "Episode length": 999, "Policy Loss": 7.211092948913574, "Value Loss": 17651.1484375, "_runtime": 9627.717052936554, "_timestamp": 1585518706.1377826, "_step": 8}
{"Episode reward": -94.93632923588649, "Episode length": 999, "Policy Loss": -1.0100361108779907, "Value Loss": 838.6143798828125, "_runtime": 9629.304091215134, "_timestamp": 1585518707.7248209, "_step": 9}
{"Episode reward": -97.38006322181991, "Episode length": 999, "Policy Loss": -56.208316802978516, "Value Loss": 8816.4296875, "_runtime": 9629.687288284302, "_timestamp": 1585518708.108018, "_step": 10}
{"Episode reward": 79.18980551172278, "Episode length": 210, "Policy Loss": 34.52927780151367, "Value Loss": 100518.9609375, "_runtime": 9631.27206158638, "_timestamp": 1585518709.6927912, "_step": 11}
{"Episode reward": -99.5590215074845, "Episode length": 999, "Policy Loss": -19.88724136352539, "Value Loss": 8207.099609375, "_runtime": 9632.85837650299, "_timestamp": 1585518711.2791061, "_step": 12}
{"Episode reward": -99.50515273256322, "Episode length": 999, "Policy Loss": -14.880234718322754, "Value Loss": 2367.283447265625, "_runtime": 9634.356622695923, "_timestamp": 1585518712.7773523, "_step": 13}
{"Episode reward": -99.49103551981834, "Episode length": 999, "Policy Loss": -16.031404495239258, "Value Loss": 43.02623748779297, "_runtime": 9634.992293834686, "_timestamp": 1585518713.4130235, "_step": 14}
{"Episode reward": 62.010012763173755, "Episode length": 380, "Policy Loss": -21.8261661529541, "Value Loss": 430.539306640625, "_runtime": 9636.014469146729, "_timestamp": 1585518714.4351988, "_step": 15}
{"Episode reward": 35.532313429526425, "Episode length": 647, "Policy Loss": -23.27740478515625, "Value Loss": 1106.7789306640625, "_runtime": 9637.551077365875, "_timestamp": 1585518715.971807, "_step": 16}
{"Episode reward": -99.80213193150563, "Episode length": 999, "Policy Loss": -28.962244033813477, "Value Loss": 1171.6680908203125, "_runtime": 9639.061077833176, "_timestamp": 1585518717.4818075, "_step": 17}
{"Episode reward": -99.57967166784083, "Episode length": 999, "Policy Loss": -29.290929794311523, "Value Loss": 1286.8604736328125, "_runtime": 9640.047938585281, "_timestamp": 1585518718.4686682, "_step": 18}
{"Episode reward": 36.98048896681805, "Episode length": 632, "Policy Loss": -37.3818359375, "Value Loss": 1616.55859375, "_runtime": 9641.644518613815, "_timestamp": 1585518720.0652483, "_step": 19}
{"Episode reward": -99.86225714692706, "Episode length": 999, "Policy Loss": -30.461488723754883, "Value Loss": 893.5023193359375, "_runtime": 9643.195040464401, "_timestamp": 1585518721.61577, "_step": 20}
{"Episode reward": -99.87329106168332, "Episode length": 999, "Policy Loss": -26.80413246154785, "Value Loss": 692.7626953125, "_runtime": 9644.271493196487, "_timestamp": 1585518722.6922228, "_step": 21}
{"Episode reward": 31.133821105648167, "Episode length": 692, "Policy Loss": -32.91448974609375, "Value Loss": 2123.0947265625, "_runtime": 9645.82248210907, "_timestamp": 1585518724.2432117, "_step": 22}
{"Episode reward": -99.61912661234244, "Episode length": 999, "Policy Loss": -23.641122817993164, "Value Loss": 181.4823455810547, "_runtime": 9647.125988483429, "_timestamp": 1585518725.5467181, "_step": 23}
{"Episode reward": 16.63114218831143, "Episode length": 836, "Policy Loss": -26.33770751953125, "Value Loss": 349.0491943359375, "_runtime": 9647.620749235153, "_timestamp": 1585518726.0414789, "_step": 24}
{"Episode reward": 70.29999999999986, "Episode length": 297, "Policy Loss": -14.3508882522583, "Value Loss": 1586.8973388671875, "_runtime": 9648.919290065765, "_timestamp": 1585518727.3400197, "_step": 25}
{"Episode reward": 16.94442317520779, "Episode length": 831, "Policy Loss": -20.846370697021484, "Value Loss": 2053.0185546875, "_runtime": 9649.397655248642, "_timestamp": 1585518727.818385, "_step": 26}
{"Episode reward": 71.89999999999988, "Episode length": 281, "Policy Loss": -25.014238357543945, "Value Loss": 126.96502685546875, "_runtime": 9650.057806491852, "_timestamp": 1585518728.4785361, "_step": 27}
{"Episode reward": 55.86720403523615, "Episode length": 442, "Policy Loss": -29.011817932128906, "Value Loss": 202.22776794433594, "_runtime": 9650.942315340042, "_timestamp": 1585518729.363045, "_step": 28}
{"Episode reward": 44.21927655035018, "Episode length": 558, "Policy Loss": -26.330577850341797, "Value Loss": 400.6568603515625, "_runtime": 9652.205623149872, "_timestamp": 1585518730.6263528, "_step": 29}
{"Episode reward": 15.523238707148067, "Episode length": 846, "Policy Loss": -20.63568878173828, "Value Loss": 289.76947021484375, "_runtime": 9653.706948518753, "_timestamp": 1585518732.1276782, "_step": 30}
{"Episode reward": -99.74010456725816, "Episode length": 999, "Policy Loss": -19.16976547241211, "Value Loss": 134.462646484375, "_runtime": 9655.228465795517, "_timestamp": 1585518733.6491954, "_step": 31}
{"Episode reward": -99.85497846246189, "Episode length": 999, "Policy Loss": -18.803424835205078, "Value Loss": 52.68464279174805, "_runtime": 9655.90919995308, "_timestamp": 1585518734.3299296, "_step": 32}
{"Episode reward": 56.49999999999966, "Episode length": 435, "Policy Loss": -28.803136825561523, "Value Loss": 331.51434326171875, "_runtime": 9657.463513612747, "_timestamp": 1585518735.8842432, "_step": 33}
{"Episode reward": -99.79481571055808, "Episode length": 999, "Policy Loss": -49.71662521362305, "Value Loss": 280.517333984375, "_runtime": 9657.960016965866, "_timestamp": 1585518736.3807466, "_step": 34}
{"Episode reward": 70.89994492521495, "Episode length": 292, "Policy Loss": -36.56745147705078, "Value Loss": 617.3875122070312, "_runtime": 9659.465607881546, "_timestamp": 1585518737.8863375, "_step": 35}
{"Episode reward": -99.86234265794488, "Episode length": 999, "Policy Loss": -16.771116256713867, "Value Loss": 8.616284370422363, "_runtime": 9660.026686668396, "_timestamp": 1585518738.4474163, "_step": 36}
{"Episode reward": 65.79999999999978, "Episode length": 342, "Policy Loss": 6.861358642578125, "Value Loss": 1850.2978515625, "_runtime": 9661.104494333267, "_timestamp": 1585518739.525224, "_step": 37}
{"Episode reward": 27.799999999999827, "Episode length": 722, "Policy Loss": -13.76764965057373, "Value Loss": 30.233837127685547, "_runtime": 9661.978944778442, "_timestamp": 1585518740.3996744, "_step": 38}
{"Episode reward": 44.299999999999486, "Episode length": 557, "Policy Loss": -1.7465627193450928, "Value Loss": 199.14781188964844, "_runtime": 9662.465473651886, "_timestamp": 1585518740.8862033, "_step": 39}
{"Episode reward": 68.49999999999983, "Episode length": 315, "Policy Loss": -20.154979705810547, "Value Loss": 136.44485473632812, "_runtime": 9662.970449209213, "_timestamp": 1585518741.3911788, "_step": 40}
{"Episode reward": 67.99999999999983, "Episode length": 320, "Policy Loss": -12.609399795532227, "Value Loss": 319.53289794921875, "_runtime": 9663.5055975914, "_timestamp": 1585518741.9263272, "_step": 41}
{"Episode reward": 65.9999999999998, "Episode length": 340, "Policy Loss": -12.778288841247559, "Value Loss": 355.4261779785156, "_runtime": 9663.776105165482, "_timestamp": 1585518742.1968348, "_step": 42}
{"Episode reward": 83.00000000000003, "Episode length": 170, "Policy Loss": -0.3532359302043915, "Value Loss": 147.34182739257812, "_runtime": 9664.134166240692, "_timestamp": 1585518742.5548959, "_step": 43}
{"Episode reward": 76.79999999999995, "Episode length": 232, "Policy Loss": 24.33974838256836, "Value Loss": 2286.70849609375, "_runtime": 9664.408359527588, "_timestamp": 1585518742.8290892, "_step": 44}
{"Episode reward": 82.50000000000003, "Episode length": 175, "Policy Loss": -38.01691818237305, "Value Loss": 203.1764678955078, "_runtime": 9664.716456651688, "_timestamp": 1585518743.1371863, "_step": 45}
{"Episode reward": 79.29999999999998, "Episode length": 207, "Policy Loss": 1.4636801481246948, "Value Loss": 724.4147338867188, "_runtime": 9665.023736000061, "_timestamp": 1585518743.4444656, "_step": 46}
{"Episode reward": 79.79999999999998, "Episode length": 202, "Policy Loss": 31.003860473632812, "Value Loss": 853.3467407226562, "_runtime": 9665.239129781723, "_timestamp": 1585518743.6598594, "_step": 47}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": 53.74028396606445, "Value Loss": 347.1820068359375, "_runtime": 9665.45156288147, "_timestamp": 1585518743.8722925, "_step": 48}
{"Episode reward": 86.34200656408777, "Episode length": 137, "Policy Loss": 17.979921340942383, "Value Loss": 150.82455444335938, "_runtime": 9665.584853887558, "_timestamp": 1585518744.0055835, "_step": 49}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -12.101140975952148, "Value Loss": 123.9667739868164, "_runtime": 9665.703701972961, "_timestamp": 1585518744.1244316, "_step": 50}
{"Episode reward": 92.70000000000002, "Episode length": 73, "Policy Loss": -5.124721527099609, "Value Loss": 128.21652221679688, "_runtime": 9665.831252098083, "_timestamp": 1585518744.2519817, "_step": 51}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": 8.410235404968262, "Value Loss": 122.85025024414062, "_runtime": 9666.064710140228, "_timestamp": 1585518744.4854398, "_step": 52}
{"Episode reward": 84.20000000000005, "Episode length": 158, "Policy Loss": -82.46405029296875, "Value Loss": 1619.14306640625, "_runtime": 9666.19056892395, "_timestamp": 1585518744.6112986, "_step": 53}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": 30.492877960205078, "Value Loss": 142.89447021484375, "_runtime": 9666.423209190369, "_timestamp": 1585518744.8439388, "_step": 54}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": -8.85644817352295, "Value Loss": 133.65997314453125, "_runtime": 9666.55931186676, "_timestamp": 1585518744.9800415, "_step": 55}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 40.258949279785156, "Value Loss": 271.0998229980469, "_runtime": 9666.692291021347, "_timestamp": 1585518745.1130207, "_step": 56}
{"Episode reward": 91.40000000000002, "Episode length": 86, "Policy Loss": 20.121124267578125, "Value Loss": 185.0055694580078, "_runtime": 9666.827908277512, "_timestamp": 1585518745.248638, "_step": 57}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 3.3201904296875, "Value Loss": 129.43270874023438, "_runtime": 9668.331678152084, "_timestamp": 1585518746.7524078, "_step": 58}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": -1.323453664779663, "Value Loss": 0.21635685861110687, "_runtime": 9668.575402975082, "_timestamp": 1585518746.9961326, "_step": 59}
{"Episode reward": 84.70000000000005, "Episode length": 153, "Policy Loss": -48.566593170166016, "Value Loss": 115.7665023803711, "_runtime": 9668.799996137619, "_timestamp": 1585518747.2207258, "_step": 60}
{"Episode reward": 84.80000000000004, "Episode length": 152, "Policy Loss": -46.76404571533203, "Value Loss": 105.05303955078125, "_runtime": 9669.089495420456, "_timestamp": 1585518747.510225, "_step": 61}
{"Episode reward": 84.8630682320949, "Episode length": 152, "Policy Loss": -35.138736724853516, "Value Loss": 83.0535888671875, "_runtime": 9669.232761383057, "_timestamp": 1585518747.653491, "_step": 62}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": -30.163894653320312, "Value Loss": 111.89016723632812, "_runtime": 9669.454078912735, "_timestamp": 1585518747.8748085, "_step": 63}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -43.945980072021484, "Value Loss": 91.87417602539062, "_runtime": 9669.685269355774, "_timestamp": 1585518748.105999, "_step": 64}
{"Episode reward": 84.80000000000004, "Episode length": 152, "Policy Loss": -30.70854377746582, "Value Loss": 78.61290740966797, "_runtime": 9670.1156001091, "_timestamp": 1585518748.5363297, "_step": 65}
{"Episode reward": 70.59999999999985, "Episode length": 294, "Policy Loss": -28.242887496948242, "Value Loss": 48.603759765625, "_runtime": 9670.435067653656, "_timestamp": 1585518748.8557973, "_step": 66}
{"Episode reward": 78.29999999999997, "Episode length": 217, "Policy Loss": -17.602567672729492, "Value Loss": 50.92088317871094, "_runtime": 9670.766323328018, "_timestamp": 1585518749.187053, "_step": 67}
{"Episode reward": 77.49999999999996, "Episode length": 225, "Policy Loss": -8.330374717712402, "Value Loss": 45.206817626953125, "_runtime": 9670.991995096207, "_timestamp": 1585518749.4127247, "_step": 68}
{"Episode reward": 85.80000000000004, "Episode length": 142, "Policy Loss": -4.1308746337890625, "Value Loss": 69.93427276611328, "_runtime": 9671.43773317337, "_timestamp": 1585518749.8584628, "_step": 69}
{"Episode reward": 70.19999999999985, "Episode length": 298, "Policy Loss": 5.326853275299072, "Value Loss": 34.956478118896484, "_runtime": 9671.776416540146, "_timestamp": 1585518750.1971462, "_step": 70}
{"Episode reward": 77.47733678666404, "Episode length": 226, "Policy Loss": 14.048657417297363, "Value Loss": 49.81696701049805, "_runtime": 9672.217500209808, "_timestamp": 1585518750.6382298, "_step": 71}
{"Episode reward": 69.89999999999985, "Episode length": 301, "Policy Loss": 13.32996940612793, "Value Loss": 40.70140838623047, "_runtime": 9672.455376148224, "_timestamp": 1585518750.8761058, "_step": 72}
{"Episode reward": 85.00000000000004, "Episode length": 150, "Policy Loss": 14.42241382598877, "Value Loss": 76.0245361328125, "_runtime": 9672.68640255928, "_timestamp": 1585518751.1071322, "_step": 73}
{"Episode reward": 85.10000000000004, "Episode length": 149, "Policy Loss": 15.637618064880371, "Value Loss": 77.55522155761719, "_runtime": 9673.024498701096, "_timestamp": 1585518751.4452283, "_step": 74}
{"Episode reward": 77.89999999999996, "Episode length": 221, "Policy Loss": 15.72510051727295, "Value Loss": 55.69036865234375, "_runtime": 9673.487825870514, "_timestamp": 1585518751.9085555, "_step": 75}
{"Episode reward": 68.39999999999984, "Episode length": 316, "Policy Loss": 20.103673934936523, "Value Loss": 45.93760681152344, "_runtime": 9673.812187194824, "_timestamp": 1585518752.2329168, "_step": 76}
{"Episode reward": 77.99999999999997, "Episode length": 220, "Policy Loss": 9.361994743347168, "Value Loss": 51.433013916015625, "_runtime": 9674.036448717117, "_timestamp": 1585518752.4571784, "_step": 77}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": 10.031074523925781, "Value Loss": 73.08687591552734, "_runtime": 9674.266112565994, "_timestamp": 1585518752.6868422, "_step": 78}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": 8.653616905212402, "Value Loss": 71.6412124633789, "_runtime": 9674.500725746155, "_timestamp": 1585518752.9214554, "_step": 79}
{"Episode reward": 85.00000000000004, "Episode length": 150, "Policy Loss": 5.843068599700928, "Value Loss": 67.9862289428711, "_runtime": 9675.527431964874, "_timestamp": 1585518753.9481616, "_step": 80}
{"Episode reward": 30.653856404777287, "Episode length": 694, "Policy Loss": 6.2813920974731445, "Value Loss": 15.841028213500977, "_runtime": 9675.858616352081, "_timestamp": 1585518754.279346, "_step": 81}
{"Episode reward": 78.59999999999997, "Episode length": 214, "Policy Loss": 1.5230121612548828, "Value Loss": 46.62474822998047, "_runtime": 9676.073504447937, "_timestamp": 1585518754.494234, "_step": 82}
{"Episode reward": 85.90000000000003, "Episode length": 141, "Policy Loss": -4.8589935302734375, "Value Loss": 70.38531494140625, "_runtime": 9676.35445022583, "_timestamp": 1585518754.7751799, "_step": 83}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": -6.612848281860352, "Value Loss": 62.00311279296875, "_runtime": 9676.794648885727, "_timestamp": 1585518755.2153785, "_step": 84}
{"Episode reward": 70.39999999999986, "Episode length": 296, "Policy Loss": -4.746787071228027, "Value Loss": 34.2440185546875, "_runtime": 9677.013867139816, "_timestamp": 1585518755.4345968, "_step": 85}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": -9.66968822479248, "Value Loss": 69.34259796142578, "_runtime": 9677.251370429993, "_timestamp": 1585518755.6721, "_step": 86}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": -12.14838981628418, "Value Loss": 65.38639068603516, "_runtime": 9677.506796836853, "_timestamp": 1585518755.9275265, "_step": 87}
{"Episode reward": 83.70000000000005, "Episode length": 163, "Policy Loss": -9.605213165283203, "Value Loss": 62.21332931518555, "_runtime": 9677.721072435379, "_timestamp": 1585518756.141802, "_step": 88}
{"Episode reward": 85.90000000000003, "Episode length": 141, "Policy Loss": -12.08952808380127, "Value Loss": 72.02828216552734, "_runtime": 9677.85158586502, "_timestamp": 1585518756.2723155, "_step": 89}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -14.896371841430664, "Value Loss": 125.2220687866211, "_runtime": 9678.083187818527, "_timestamp": 1585518756.5039175, "_step": 90}
{"Episode reward": 84.70000000000005, "Episode length": 153, "Policy Loss": -10.958535194396973, "Value Loss": 66.31232452392578, "_runtime": 9678.515422821045, "_timestamp": 1585518756.9361525, "_step": 91}
{"Episode reward": 70.69999999999986, "Episode length": 293, "Policy Loss": -7.262328624725342, "Value Loss": 35.23827362060547, "_runtime": 9678.732461452484, "_timestamp": 1585518757.153191, "_step": 92}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": -11.149519920349121, "Value Loss": 69.77910614013672, "_runtime": 9679.186398744583, "_timestamp": 1585518757.6071284, "_step": 93}
{"Episode reward": 68.99999999999983, "Episode length": 310, "Policy Loss": -10.27723217010498, "Value Loss": 33.807579040527344, "_runtime": 9679.415718317032, "_timestamp": 1585518757.836448, "_step": 94}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -8.881692886352539, "Value Loss": 69.36749267578125, "_runtime": 9679.63760471344, "_timestamp": 1585518758.0583344, "_step": 95}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -8.218294143676758, "Value Loss": 68.27799987792969, "_runtime": 9679.86490368843, "_timestamp": 1585518758.2856333, "_step": 96}
{"Episode reward": 85.80000000000004, "Episode length": 142, "Policy Loss": -7.094954967498779, "Value Loss": 69.95511627197266, "_runtime": 9680.080009698868, "_timestamp": 1585518758.5007393, "_step": 97}
{"Episode reward": 85.80000000000004, "Episode length": 142, "Policy Loss": -7.400044918060303, "Value Loss": 70.04926300048828, "gradients/fc.2.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [-73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625, -73584.9765625]}, "gradients/fc.2.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0], "bins": [-4794858.0, -4718788.5, -4642719.0, -4566649.5, -4490580.5, -4414511.0, -4338441.5, -4262372.0, -4186302.5, -4110233.25, -4034163.75, -3958094.5, -3882025.0, -3805955.5, -3729886.0, -3653816.5, -3577747.25, -3501678.0, -3425608.5, -3349539.0, -3273469.5, -3197400.0, -3121330.75, -3045261.25, -2969192.0, -2893122.5, -2817053.0, -2740983.5, -2664914.25, -2588844.75, -2512775.25, -2436706.0, -2360636.5, -2284567.0, -2208497.75, -2132428.25, -2056358.75, -1980289.5, -1904220.0, -1828150.5, -1752081.0, -1676011.75, -1599942.25, -1523872.75, -1447803.5, -1371734.0, -1295664.5, -1219595.25, -1143525.75, -1067456.25, -991387.0, -915317.5, -839248.0, -763178.75, -687109.25, -611039.75, -534970.5, -458901.0, -382831.5, -306762.0, -230692.5, -154623.5, -78554.0, -2484.5, 73585.0]}, "gradients/fc.1.bias": {"_type": "histogram", "values": [3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], "bins": [-122977.7734375, -112102.2265625, -101226.671875, -90351.125, -79475.578125, -68600.03125, -57724.48046875, -46848.9296875, -35973.3828125, -25097.8359375, -14222.2890625, -3346.734375, 7528.8125, 18404.3671875, 29279.9140625, 40155.4609375, 51031.0078125, 61906.5546875, 72782.1015625, 83657.6484375, 94533.1953125, 105408.7578125, 116284.3046875, 127159.8515625, 138035.40625, 148910.9375, 159786.5, 170662.03125, 181537.59375, 192413.125, 203288.6875, 214164.21875, 225039.78125, 235915.34375, 246790.875, 257666.4375, 268541.96875, 279417.53125, 290293.0625, 301168.625, 312044.15625, 322919.71875, 333795.28125, 344670.8125, 355546.375, 366421.90625, 377297.46875, 388173.0, 399048.5625, 409924.09375, 420799.65625, 431675.21875, 442550.78125, 453426.28125, 464301.84375, 475177.40625, 486052.96875, 496928.53125, 507804.03125, 518679.59375, 529555.1875, 540430.75, 551306.25, 562181.8125, 573057.375]}, "gradients/fc.1.weight": {"_type": "histogram", "values": [5.0, 2.0, 4.0, 15.0, 43.0, 25.0, 31.0, 8.0, 1.0, 5.0, 8.0, 216.0, 71.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 3.0, 3.0, 2.0, 1.0, 3.0, 6.0, 4.0, 5.0, 3.0, 3.0, 3.0, 4.0, 3.0, 1.0, 2.0, 4.0, 3.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 1.0], "bins": [-940051.625, -856918.0625, -773784.5, -690651.0, -607517.4375, -524383.875, -441250.34375, -358116.8125, -274983.25, -191849.6875, -108716.125, -25582.625, 57550.9375, 140684.5, 223818.0, 306951.625, 390085.125, 473218.625, 556352.25, 639485.75, 722619.375, 805752.875, 888886.375, 972020.0, 1055153.5, 1138287.0, 1221420.625, 1304554.125, 1387687.625, 1470821.125, 1553954.875, 1637088.375, 1720221.875, 1803355.375, 1886488.875, 1969622.625, 2052756.125, 2135889.5, 2219023.0, 2302156.5, 2385290.5, 2468424.0, 2551557.5, 2634691.0, 2717824.5, 2800958.0, 2884091.5, 2967225.0, 3050358.5, 3133492.0, 3216625.5, 3299759.5, 3382893.0, 3466026.5, 3549160.0, 3632293.5, 3715427.0, 3798560.5, 3881694.0, 3964828.0, 4047961.5, 4131095.0, 4214228.5, 4297362.0, 4380495.5]}, "gradients/fc.0.bias": {"_type": "histogram", "values": [2.0, 2.0, 5.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 9.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0], "bins": [-17423622.0, -16768066.0, -16112510.0, -15456954.0, -14801398.0, -14145842.0, -13490286.0, -12834730.0, -12179174.0, -11523618.0, -10868062.0, -10212506.0, -9556950.0, -8901394.0, -8245838.0, -7590282.0, -6934726.0, -6279170.0, -5623614.0, -4968058.0, -4312502.0, -3656946.0, -3001390.0, -2345834.0, -1690278.0, -1034722.0, -379166.0, 276390.0, 931946.0, 1587502.0, 2243058.0, 2898614.0, 3554170.0, 4209726.0, 4865282.0, 5520838.0, 6176394.0, 6831950.0, 7487506.0, 8143062.0, 8798618.0, 9454174.0, 10109730.0, 10765286.0, 11420842.0, 12076398.0, 12731954.0, 13387510.0, 14043066.0, 14698622.0, 15354178.0, 16009734.0, 16665290.0, 17320846.0, 17976402.0, 18631958.0, 19287514.0, 19943070.0, 20598626.0, 21254182.0, 21909738.0, 22565294.0, 23220850.0, 23876406.0, 24531962.0]}, "gradients/fc.0.weight": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 4.0, 2.0, 1.0, 2.0, 20.0, 1.0, 0.0, 5.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 5.0, 2.0, 2.0], "bins": [-29802784.0, -29006378.0, -28209972.0, -27413568.0, -26617162.0, -25820756.0, -25024350.0, -24227944.0, -23431540.0, -22635134.0, -21838728.0, -21042322.0, -20245916.0, -19449512.0, -18653104.0, -17856700.0, -17060294.0, -16263888.0, -15467483.0, -14671077.0, -13874672.0, -13078266.0, -12281860.0, -11485454.0, -10689048.0, -9892644.0, -9096238.0, -8299832.0, -7503426.0, -6707020.0, -5910616.0, -5114210.0, -4317804.0, -3521398.0, -2724992.0, -1928588.0, -1132182.0, -335776.0, 460630.0, 1257036.0, 2053440.0, 2849846.0, 3646252.0, 4442656.0, 5239064.0, 6035468.0, 6831876.0, 7628280.0, 8424688.0, 9221092.0, 10017496.0, 10813904.0, 11610308.0, 12406716.0, 13203120.0, 13999524.0, 14795932.0, 15592336.0, 16388744.0, 17185148.0, 17981552.0, 18777960.0, 19574364.0, 20370772.0, 21167176.0]}, "_runtime": 9680.297360181808, "_timestamp": 1585518758.7180898, "_step": 98}
{"Episode reward": 85.90000000000003, "Episode length": 141, "Policy Loss": -7.071531295776367, "Value Loss": 70.53474426269531, "_runtime": 9680.514442682266, "_timestamp": 1585518758.9351723, "_step": 99}
{"Episode reward": 85.80000000000004, "Episode length": 142, "Policy Loss": -6.4333953857421875, "Value Loss": 69.97843170166016, "_runtime": 9680.722409248352, "_timestamp": 1585518759.143139, "_step": 100}
{"Episode reward": 86.46241083263853, "Episode length": 136, "Policy Loss": -5.941605091094971, "Value Loss": 72.97874450683594, "_runtime": 9680.957114696503, "_timestamp": 1585518759.3778443, "_step": 101}
{"Episode reward": 84.56452013216916, "Episode length": 155, "Policy Loss": -4.412356853485107, "Value Loss": 64.08232116699219, "_runtime": 9681.184449195862, "_timestamp": 1585518759.6051788, "_step": 102}
{"Episode reward": 85.10000000000004, "Episode length": 149, "Policy Loss": -3.741014242172241, "Value Loss": 66.60679626464844, "_runtime": 9681.414950609207, "_timestamp": 1585518759.8356802, "_step": 103}
{"Episode reward": 84.92644090056423, "Episode length": 151, "Policy Loss": -4.521390438079834, "Value Loss": 66.03535461425781, "_runtime": 9681.640196800232, "_timestamp": 1585518760.0609264, "_step": 104}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -3.5283191204071045, "Value Loss": 67.28443145751953, "_runtime": 9681.897820711136, "_timestamp": 1585518760.3185503, "_step": 105}
{"Episode reward": 85.55634580728952, "Episode length": 145, "Policy Loss": -4.071422100067139, "Value Loss": 68.57526397705078, "_runtime": 9682.11228108406, "_timestamp": 1585518760.5330107, "_step": 106}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": -3.428454637527466, "Value Loss": 70.99059295654297, "_runtime": 9682.24642109871, "_timestamp": 1585518760.6671507, "_step": 107}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": -11.05562686920166, "Value Loss": 119.62938690185547, "_runtime": 9682.475271701813, "_timestamp": 1585518760.8960013, "_step": 108}
{"Episode reward": 84.90000000000005, "Episode length": 151, "Policy Loss": -3.5233154296875, "Value Loss": 66.17127227783203, "_runtime": 9682.687842607498, "_timestamp": 1585518761.1085722, "_step": 109}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": -2.593104600906372, "Value Loss": 71.01323699951172, "_runtime": 9682.806539297104, "_timestamp": 1585518761.227269, "_step": 110}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -10.579008102416992, "Value Loss": 131.23507690429688, "_runtime": 9683.038029432297, "_timestamp": 1585518761.458759, "_step": 111}
{"Episode reward": 84.70000000000005, "Episode length": 153, "Policy Loss": -3.3253707885742188, "Value Loss": 65.30901336669922, "_runtime": 9683.390847921371, "_timestamp": 1585518761.8115776, "_step": 112}
{"Episode reward": 76.29999999999994, "Episode length": 237, "Policy Loss": 0.22688551247119904, "Value Loss": 42.291900634765625, "_runtime": 9683.511073589325, "_timestamp": 1585518761.9318032, "_step": 113}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -11.323534965515137, "Value Loss": 129.87930297851562, "_runtime": 9683.753769636154, "_timestamp": 1585518762.1744993, "_step": 114}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": -2.8503270149230957, "Value Loss": 61.970428466796875, "_runtime": 9683.882804870605, "_timestamp": 1585518762.3035345, "_step": 115}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -11.824614524841309, "Value Loss": 131.69671630859375, "_runtime": 9684.083979129791, "_timestamp": 1585518762.5047088, "_step": 116}
{"Episode reward": 86.60000000000004, "Episode length": 134, "Policy Loss": -4.055669784545898, "Value Loss": 74.11536407470703, "_runtime": 9684.206044435501, "_timestamp": 1585518762.626774, "_step": 117}
{"Episode reward": 92.70000000000002, "Episode length": 73, "Policy Loss": -12.094536781311035, "Value Loss": 135.20745849609375, "_runtime": 9684.421743392944, "_timestamp": 1585518762.842473, "_step": 118}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -4.836562633514404, "Value Loss": 69.28353881835938, "_runtime": 9684.656399011612, "_timestamp": 1585518763.0771286, "_step": 119}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": -3.4174304008483887, "Value Loss": 63.85500717163086, "_runtime": 9684.855566740036, "_timestamp": 1585518763.2762964, "_step": 120}
{"Episode reward": 86.90000000000003, "Episode length": 131, "Policy Loss": -4.583011150360107, "Value Loss": 75.60929870605469, "_runtime": 9685.061558961868, "_timestamp": 1585518763.4822886, "_step": 121}
{"Episode reward": 86.50000000000004, "Episode length": 135, "Policy Loss": -4.709864139556885, "Value Loss": 73.33969116210938, "_runtime": 9685.271367788315, "_timestamp": 1585518763.6920974, "_step": 122}
{"Episode reward": 86.40000000000003, "Episode length": 136, "Policy Loss": -4.9082746505737305, "Value Loss": 73.0551986694336, "_runtime": 9685.492626667023, "_timestamp": 1585518763.9133563, "_step": 123}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -4.460463523864746, "Value Loss": 68.11792755126953, "_runtime": 9685.842670202255, "_timestamp": 1585518764.2633998, "_step": 124}
{"Episode reward": 76.29999999999994, "Episode length": 237, "Policy Loss": -1.0834848880767822, "Value Loss": 42.194557189941406, "_runtime": 9685.959789276123, "_timestamp": 1585518764.380519, "_step": 125}
{"Episode reward": 92.90000000000002, "Episode length": 71, "Policy Loss": -12.787251472473145, "Value Loss": 138.9063720703125, "_runtime": 9686.293132781982, "_timestamp": 1585518764.7138624, "_step": 126}
{"Episode reward": 77.39999999999995, "Episode length": 226, "Policy Loss": -1.601377010345459, "Value Loss": 44.117340087890625, "_runtime": 9686.52168250084, "_timestamp": 1585518764.9424121, "_step": 127}
{"Episode reward": 85.30000000000004, "Episode length": 147, "Policy Loss": -4.773560523986816, "Value Loss": 67.50299072265625, "_runtime": 9686.73302602768, "_timestamp": 1585518765.1537557, "_step": 128}
{"Episode reward": 85.90000000000003, "Episode length": 141, "Policy Loss": -4.579268455505371, "Value Loss": 70.28376770019531, "_runtime": 9686.963802099228, "_timestamp": 1585518765.3845317, "_step": 129}
{"Episode reward": 85.19063268286885, "Episode length": 149, "Policy Loss": -3.7800662517547607, "Value Loss": 66.51557159423828, "_runtime": 9687.176020145416, "_timestamp": 1585518765.5967498, "_step": 130}
{"Episode reward": 86.19925799557099, "Episode length": 139, "Policy Loss": -5.2306413650512695, "Value Loss": 71.46342468261719, "_runtime": 9687.385909795761, "_timestamp": 1585518765.8066394, "_step": 131}
{"Episode reward": 86.50000000000004, "Episode length": 135, "Policy Loss": -5.8151702880859375, "Value Loss": 73.44652557373047, "_runtime": 9687.59667134285, "_timestamp": 1585518766.017401, "_step": 132}
{"Episode reward": 86.30000000000004, "Episode length": 137, "Policy Loss": -5.518412113189697, "Value Loss": 72.47335052490234, "_runtime": 9687.917079687119, "_timestamp": 1585518766.3378093, "_step": 133}
{"Episode reward": 78.69999999999997, "Episode length": 213, "Policy Loss": -2.888526678085327, "Value Loss": 46.626197814941406, "_runtime": 9688.121052265167, "_timestamp": 1585518766.541782, "_step": 134}
{"Episode reward": 86.98877193778756, "Episode length": 131, "Policy Loss": -6.4801483154296875, "Value Loss": 75.7032699584961, "_runtime": 9688.325740814209, "_timestamp": 1585518766.7464705, "_step": 135}
{"Episode reward": 86.70000000000003, "Episode length": 133, "Policy Loss": -5.354803562164307, "Value Loss": 74.36695861816406, "_runtime": 9688.543978691101, "_timestamp": 1585518766.9647083, "_step": 136}
{"Episode reward": 86.10000000000004, "Episode length": 139, "Policy Loss": -4.805552005767822, "Value Loss": 71.18262481689453, "_runtime": 9688.66501379013, "_timestamp": 1585518767.0857434, "_step": 137}
{"Episode reward": 92.70000000000002, "Episode length": 73, "Policy Loss": -12.90064811706543, "Value Loss": 135.19876098632812, "_runtime": 9688.86366224289, "_timestamp": 1585518767.2843919, "_step": 138}
{"Episode reward": 87.10000000000004, "Episode length": 129, "Policy Loss": -6.2983527183532715, "Value Loss": 76.70881652832031, "_runtime": 9689.175145864487, "_timestamp": 1585518767.5958755, "_step": 139}
{"Episode reward": 79.29999999999998, "Episode length": 207, "Policy Loss": -2.697258949279785, "Value Loss": 47.97979736328125, "_runtime": 9689.377238988876, "_timestamp": 1585518767.7979686, "_step": 140}
{"Episode reward": 86.80000000000004, "Episode length": 132, "Policy Loss": -5.3496246337890625, "Value Loss": 74.91825866699219, "_runtime": 9689.502596616745, "_timestamp": 1585518767.9233263, "_step": 141}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -11.974576950073242, "Value Loss": 128.25732421875, "_runtime": 9689.737144708633, "_timestamp": 1585518768.1578743, "_step": 142}
{"Episode reward": 84.90000000000005, "Episode length": 151, "Policy Loss": -2.226384401321411, "Value Loss": 65.36883544921875, "_runtime": 9689.948439121246, "_timestamp": 1585518768.3691688, "_step": 143}
{"Episode reward": 86.30000000000004, "Episode length": 137, "Policy Loss": -3.5677826404571533, "Value Loss": 72.10356903076172, "_runtime": 9690.17212843895, "_timestamp": 1585518768.592858, "_step": 144}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -1.940002679824829, "Value Loss": 66.82691955566406, "_runtime": 9690.370588302612, "_timestamp": 1585518768.791318, "_step": 145}
{"Episode reward": 87.20000000000003, "Episode length": 128, "Policy Loss": -5.332237243652344, "Value Loss": 77.32161712646484, "_runtime": 9690.579827308655, "_timestamp": 1585518769.000557, "_step": 146}
{"Episode reward": 86.40000000000003, "Episode length": 136, "Policy Loss": -3.500629186630249, "Value Loss": 72.64994049072266, "_runtime": 9690.79637670517, "_timestamp": 1585518769.2171063, "_step": 147}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": -3.2826483249664307, "Value Loss": 70.8431167602539, "_runtime": 9691.016361951828, "_timestamp": 1585518769.4370916, "_step": 148}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -1.8813002109527588, "Value Loss": 68.52432250976562, "_runtime": 9691.346980810165, "_timestamp": 1585518769.7677104, "_step": 149}
{"Episode reward": 77.99999999999997, "Episode length": 220, "Policy Loss": 1.46254563331604, "Value Loss": 45.219451904296875, "_runtime": 9691.552067279816, "_timestamp": 1585518769.972797, "_step": 150}
{"Episode reward": 86.70000000000003, "Episode length": 133, "Policy Loss": -4.8716630935668945, "Value Loss": 74.5063705444336, "_runtime": 9691.759219884872, "_timestamp": 1585518770.1799495, "_step": 151}
{"Episode reward": 86.60000000000004, "Episode length": 134, "Policy Loss": -4.872646331787109, "Value Loss": 73.78787994384766, "_runtime": 9691.959088563919, "_timestamp": 1585518770.3798182, "_step": 152}
{"Episode reward": 87.40000000000003, "Episode length": 126, "Policy Loss": -4.656501293182373, "Value Loss": 78.16577911376953, "_runtime": 9692.083868026733, "_timestamp": 1585518770.5045977, "_step": 153}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -10.978154182434082, "Value Loss": 129.6886444091797, "_runtime": 9692.292518854141, "_timestamp": 1585518770.7132485, "_step": 154}
{"Episode reward": 86.40000000000003, "Episode length": 136, "Policy Loss": -3.4210829734802246, "Value Loss": 72.47137451171875, "_runtime": 9692.496709346771, "_timestamp": 1585518770.917439, "_step": 155}
{"Episode reward": 86.70000000000003, "Episode length": 133, "Policy Loss": -2.0272951126098633, "Value Loss": 73.7989730834961, "_runtime": 9692.700670480728, "_timestamp": 1585518771.1214, "_step": 156}
{"Episode reward": 86.60000000000004, "Episode length": 134, "Policy Loss": -2.1766326427459717, "Value Loss": 73.34317779541016, "_runtime": 9692.906872272491, "_timestamp": 1585518771.327602, "_step": 157}
{"Episode reward": 86.60000000000004, "Episode length": 134, "Policy Loss": -2.3757290840148926, "Value Loss": 73.48543548583984, "_runtime": 9693.124395132065, "_timestamp": 1585518771.5451248, "_step": 158}
{"Episode reward": 85.80000000000004, "Episode length": 142, "Policy Loss": -1.556540608406067, "Value Loss": 69.54926300048828, "_runtime": 9693.336526870728, "_timestamp": 1585518771.7572565, "_step": 159}
{"Episode reward": 86.30000000000004, "Episode length": 137, "Policy Loss": -0.44238901138305664, "Value Loss": 71.65593719482422, "_runtime": 9693.554786682129, "_timestamp": 1585518771.9755163, "_step": 160}
{"Episode reward": 85.70000000000005, "Episode length": 143, "Policy Loss": -1.4090406894683838, "Value Loss": 69.26502990722656, "_runtime": 9693.786571264267, "_timestamp": 1585518772.207301, "_step": 161}
{"Episode reward": 84.80000000000004, "Episode length": 152, "Policy Loss": 0.46547237038612366, "Value Loss": 64.72576904296875, "_runtime": 9694.02446103096, "_timestamp": 1585518772.4451907, "_step": 162}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": 1.047981858253479, "Value Loss": 63.36962127685547, "_runtime": 9694.244276762009, "_timestamp": 1585518772.6650064, "_step": 163}
{"Episode reward": 85.70000000000005, "Episode length": 143, "Policy Loss": 0.8040902614593506, "Value Loss": 68.60995483398438, "_runtime": 9694.471321582794, "_timestamp": 1585518772.8920512, "_step": 164}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": 0.8311620354652405, "Value Loss": 66.44276428222656, "_runtime": 9694.597935438156, "_timestamp": 1585518773.018665, "_step": 165}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -7.634620189666748, "Value Loss": 128.04530334472656, "_runtime": 9694.820561647415, "_timestamp": 1585518773.2412913, "_step": 166}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": 4.607511520385742, "Value Loss": 31.186189651489258, "_runtime": 9695.06974196434, "_timestamp": 1585518773.4904716, "_step": 167}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": 1.9361751079559326, "Value Loss": 62.792625427246094, "_runtime": 9695.2902572155, "_timestamp": 1585518773.7109869, "_step": 168}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": 25.810903549194336, "Value Loss": 282.25146484375, "_runtime": 9695.509139060974, "_timestamp": 1585518773.9298687, "_step": 169}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": -1.5193684101104736, "Value Loss": 70.0299072265625, "_runtime": 9695.753458023071, "_timestamp": 1585518774.1741877, "_step": 170}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": -2.238677501678467, "Value Loss": 61.354347229003906, "_runtime": 9695.881717920303, "_timestamp": 1585518774.3024476, "_step": 171}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -11.307450294494629, "Value Loss": 124.92787170410156, "_runtime": 9696.1028778553, "_timestamp": 1585518774.5236075, "_step": 172}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -5.652640342712402, "Value Loss": 68.02210998535156, "_runtime": 9696.340020179749, "_timestamp": 1585518774.7607498, "_step": 173}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": -5.612777233123779, "Value Loss": 63.81690216064453, "_runtime": 9696.572921991348, "_timestamp": 1585518774.9936516, "_step": 174}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": -6.383457183837891, "Value Loss": 64.7562255859375, "_runtime": 9696.822795629501, "_timestamp": 1585518775.2435253, "_step": 175}
{"Episode reward": 83.60000000000005, "Episode length": 164, "Policy Loss": -5.649003028869629, "Value Loss": 60.81072998046875, "_runtime": 9697.052560091019, "_timestamp": 1585518775.4732897, "_step": 176}
{"Episode reward": 85.00000000000004, "Episode length": 150, "Policy Loss": -6.1289496421813965, "Value Loss": 66.38209533691406, "_runtime": 9697.28943157196, "_timestamp": 1585518775.7101612, "_step": 177}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": -5.368597030639648, "Value Loss": 64.5337905883789, "_runtime": 9697.424664735794, "_timestamp": 1585518775.8453944, "_step": 178}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": -10.927178382873535, "Value Loss": 120.59677124023438, "_runtime": 9697.562848806381, "_timestamp": 1585518775.9835784, "_step": 179}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -9.8931884765625, "Value Loss": 115.97276306152344, "_runtime": 9697.789494514465, "_timestamp": 1585518776.2102242, "_step": 180}
{"Episode reward": 85.30000000000004, "Episode length": 147, "Policy Loss": -2.9918978214263916, "Value Loss": 66.92549133300781, "_runtime": 9697.924157619476, "_timestamp": 1585518776.3448873, "_step": 181}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -8.363994598388672, "Value Loss": 115.86155700683594, "_runtime": 9698.054017305374, "_timestamp": 1585518776.474747, "_step": 182}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": -7.592146873474121, "Value Loss": 119.24765014648438, "_runtime": 9698.196295499802, "_timestamp": 1585518776.6170251, "_step": 183}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -7.318310260772705, "Value Loss": 111.7088851928711, "_runtime": 9698.422402858734, "_timestamp": 1585518776.8431325, "_step": 184}
{"Episode reward": 85.00000000000004, "Episode length": 150, "Policy Loss": 2.3127176761627197, "Value Loss": 65.31318664550781, "_runtime": 9698.645910978317, "_timestamp": 1585518777.0666406, "_step": 185}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": 3.936807632446289, "Value Loss": 65.8291244506836, "_runtime": 9698.780488967896, "_timestamp": 1585518777.2012186, "_step": 186}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -3.7682113647460938, "Value Loss": 114.27300262451172, "_runtime": 9699.002575397491, "_timestamp": 1585518777.423305, "_step": 187}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": 5.488963603973389, "Value Loss": 67.00598907470703, "_runtime": 9699.24552154541, "_timestamp": 1585518777.6662512, "_step": 188}
{"Episode reward": 84.10000000000005, "Episode length": 159, "Policy Loss": 6.5871782302856445, "Value Loss": 62.476016998291016, "_runtime": 9699.490387439728, "_timestamp": 1585518777.911117, "_step": 189}
{"Episode reward": 83.80000000000004, "Episode length": 162, "Policy Loss": 8.934914588928223, "Value Loss": 60.71565628051758, "_runtime": 9699.736067771912, "_timestamp": 1585518778.1567974, "_step": 190}
{"Episode reward": 83.80000000000004, "Episode length": 162, "Policy Loss": 9.24346923828125, "Value Loss": 60.551204681396484, "_runtime": 9699.881359100342, "_timestamp": 1585518778.3020887, "_step": 191}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": -0.5837720632553101, "Value Loss": 109.2117691040039, "_runtime": 9700.018975496292, "_timestamp": 1585518778.4397051, "_step": 192}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 0.8758950233459473, "Value Loss": 113.68616485595703, "_runtime": 9700.15214896202, "_timestamp": 1585518778.5728786, "_step": 193}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": 0.24376191198825836, "Value Loss": 118.9429702758789, "_runtime": 9700.286266803741, "_timestamp": 1585518778.7069964, "_step": 194}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 1.6705044507980347, "Value Loss": 114.16624450683594, "_runtime": 9700.423153162003, "_timestamp": 1585518778.8438828, "_step": 195}
{"Episode reward": 91.39731416099548, "Episode length": 87, "Policy Loss": -1.7200347185134888, "Value Loss": 111.91887664794922, "_runtime": 9700.561519145966, "_timestamp": 1585518778.9822488, "_step": 196}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -1.6582568883895874, "Value Loss": 109.82780456542969, "_runtime": 9700.795563220978, "_timestamp": 1585518779.2162929, "_step": 197}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": 7.501948356628418, "Value Loss": 62.96630859375, "gradients/fc.2.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974, 0.0565478540956974]}, "gradients/fc.2.weight": {"_type": "histogram", "values": [5.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.0565478540956974, 0.08991427719593048, 0.23637640476226807, 0.38283851742744446, 0.5293006300926208, 0.6757627725601196, 0.8222248554229736, 0.9686870574951172, 1.1151491403579712, 1.2616112232208252, 1.4080734252929688, 1.5545355081558228, 1.7009975910186768, 1.8474597930908203, 1.9939219951629639, 2.1403839588165283, 2.286846160888672, 2.4333083629608154, 2.57977032661438, 2.7262325286865234, 2.872694730758667, 3.0191566944122314, 3.165618896484375, 3.3120810985565186, 3.458543062210083, 3.6050052642822266, 3.75146746635437, 3.8979296684265137, 4.044392108917236, 4.190854072570801, 4.337316036224365, 4.483778476715088, 4.630240440368652, 4.776702404022217, 4.9231648445129395, 5.069626808166504, 5.216088771820068, 5.362551212310791, 5.5090131759643555, 5.65547513961792, 5.801937580108643, 5.948399543762207, 6.0948615074157715, 6.241323947906494, 6.387785911560059, 6.534247875213623, 6.680710315704346, 6.82717227935791, 6.973634243011475, 7.120096683502197, 7.266558647155762, 7.413021087646484, 7.559483051300049, 7.705945014953613, 7.852407455444336, 7.9988694190979, 8.145331382751465, 8.291792869567871, 8.438255310058594, 8.584717750549316, 8.731179237365723, 8.877641677856445, 9.024104118347168, 9.170565605163574, 9.317028045654297]}, "gradients/fc.1.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 7.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 3.0], "bins": [-0.46083927154541016, -0.4521231949329376, -0.4434071183204651, -0.43469107151031494, -0.4259749948978424, -0.4172589182853699, -0.4085428714752197, -0.3998267948627472, -0.39111071825027466, -0.3823946416378021, -0.3736785650253296, -0.36496251821517944, -0.3562464416027069, -0.3475303649902344, -0.33881431818008423, -0.3300982415676117, -0.32138216495513916, -0.3126660883426666, -0.3039500117301941, -0.29523396492004395, -0.2865178883075714, -0.2778018116950989, -0.26908576488494873, -0.2603696882724762, -0.25165361166000366, -0.24293753504753113, -0.2342214733362198, -0.22550541162490845, -0.2167893350124359, -0.20807325839996338, -0.19935721158981323, -0.1906411349773407, -0.18192505836486816, -0.17320898175239563, -0.1644929051399231, -0.15577685832977295, -0.14706078171730042, -0.13834470510482788, -0.12962865829467773, -0.1209125816822052, -0.11219650506973267, -0.10348042845726013, -0.0947643518447876, -0.08604830503463745, -0.07733222842216492, -0.06861615180969238, -0.059900104999542236, -0.0511840283870697, -0.04246795177459717, -0.033751875162124634, -0.0250357985496521, -0.016319751739501953, -0.007603675127029419, 0.0011124014854431152, 0.009828448295593262, 0.018544524908065796, 0.02726060152053833, 0.035976678133010864, 0.0446927547454834, 0.053408801555633545, 0.06212484836578369, 0.07084095478057861, 0.07955700159072876, 0.08827310800552368, 0.09698915481567383]}, "gradients/fc.1.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 5.0, 4.0, 0.0, 0.0, 1.0, 3.0, 3.0, 1.0, 20.0, 79.0, 243.0, 12.0, 7.0, 22.0, 16.0, 4.0, 15.0, 9.0, 20.0, 18.0, 8.0], "bins": [-3.77500581741333, -3.7039761543273926, -3.632946491241455, -3.5619168281555176, -3.49088716506958, -3.4198575019836426, -3.348827838897705, -3.2777984142303467, -3.206768751144409, -3.1357390880584717, -3.064709424972534, -2.9936797618865967, -2.922650098800659, -2.8516204357147217, -2.780590772628784, -2.709561347961426, -2.6385316848754883, -2.567502021789551, -2.4964723587036133, -2.425442695617676, -2.3544130325317383, -2.283383369445801, -2.2123537063598633, -2.141324043273926, -2.0702943801879883, -1.9992648363113403, -1.9282351732254028, -1.8572055101394653, -1.7861758470535278, -1.7151463031768799, -1.6441166400909424, -1.5730869770050049, -1.5020573139190674, -1.4310276508331299, -1.3599979877471924, -1.2889683246612549, -1.2179386615753174, -1.1469089984893799, -1.0758795738220215, -1.004849910736084, -0.9338202476501465, -0.862790584564209, -0.7917609214782715, -0.720731258392334, -0.6497015953063965, -0.578671932220459, -0.5076422691345215, -0.436612606048584, -0.3655829429626465, -0.2945535182952881, -0.22352385520935059, -0.15249419212341309, -0.08146452903747559, -0.010434865951538086, 0.060594797134399414, 0.13162446022033691, 0.20265412330627441, 0.2736835479736328, 0.3447132110595703, 0.4157428741455078, 0.4867725372314453, 0.5578022003173828, 0.6288318634033203, 0.6998615264892578, 0.7708911895751953]}, "gradients/fc.0.bias": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 5.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0], "bins": [-6.764185905456543, -6.595214366912842, -6.426242828369141, -6.2572712898254395, -6.088299751281738, -5.919327735900879, -5.750356197357178, -5.581384658813477, -5.412413120269775, -5.243441581726074, -5.074470043182373, -4.905498504638672, -4.7365264892578125, -4.5675554275512695, -4.39858341217041, -4.229611873626709, -4.060640335083008, -3.8916687965393066, -3.7226972579956055, -3.553725481033325, -3.384753942489624, -3.215782403945923, -3.0468106269836426, -2.8778390884399414, -2.7088675498962402, -2.539896011352539, -2.370924472808838, -2.2019529342651367, -2.0329809188842773, -1.8640093803405762, -1.695037841796875, -1.5260663032531738, -1.3570947647094727, -1.1881232261657715, -1.0191516876220703, -0.8501801490783691, -0.681208610534668, -0.5122365951538086, -0.3432650566101074, -0.17429351806640625, -0.005321979522705078, 0.1636495590209961, 0.33262109756469727, 0.5015926361083984, 0.6705646514892578, 0.839536190032959, 1.0085077285766602, 1.1774792671203613, 1.3464508056640625, 1.5154228210449219, 1.6843938827514648, 1.8533658981323242, 2.022336959838867, 2.1913089752197266, 2.3602800369262695, 2.529252052307129, 2.6982240676879883, 2.8671951293945312, 3.0361671447753906, 3.2051382064819336, 3.374110221862793, 3.543081283569336, 3.7120532989501953, 3.8810243606567383, 4.049996376037598]}, "gradients/fc.0.weight": {"_type": "histogram", "values": [3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 10.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], "bins": [-6.809220790863037, -6.518362998962402, -6.227505683898926, -5.936648368835449, -5.6457905769348145, -5.35493278503418, -5.064075469970703, -4.773218154907227, -4.482360363006592, -4.191502571105957, -3.9006452560424805, -3.609787702560425, -3.318930149078369, -3.0280725955963135, -2.737215042114258, -2.446357250213623, -2.1554999351501465, -1.86464262008667, -1.5737848281860352, -1.2829270362854004, -0.9920697212219238, -0.7012124061584473, -0.4103546142578125, -0.11949682235717773, 0.17136049270629883, 0.4622178077697754, 0.7530755996704102, 1.043933391571045, 1.3347907066345215, 1.625648021697998, 1.916506290435791, 2.2073636054992676, 2.498220920562744, 2.7890782356262207, 3.0799355506896973, 3.3707938194274902, 3.661651134490967, 3.9525084495544434, 4.243366718292236, 4.534224033355713, 4.8250813484191895, 5.115938663482666, 5.406795978546143, 5.6976542472839355, 5.988511562347412, 6.279368877410889, 6.570227146148682, 6.861084461212158, 7.151941776275635, 7.442799091339111, 7.733656406402588, 8.024515151977539, 8.315372467041016, 8.606229782104492, 8.897087097167969, 9.187944412231445, 9.478801727294922, 9.769659042358398, 10.060516357421875, 10.351373672485352, 10.642232894897461, 10.933090209960938, 11.223947525024414, 11.51480484008789, 11.805662155151367]}, "_runtime": 9700.928974866867, "_timestamp": 1585518779.3497045, "_step": 198}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 3.251851797103882, "Value Loss": 113.64390563964844, "_runtime": 9701.172214984894, "_timestamp": 1585518779.5929446, "_step": 199}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": 9.741756439208984, "Value Loss": 60.701377868652344, "_runtime": 9701.418830394745, "_timestamp": 1585518779.83956, "_step": 200}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": 8.649453163146973, "Value Loss": 60.3740119934082, "_runtime": 9701.561873912811, "_timestamp": 1585518779.9826035, "_step": 201}
{"Episode reward": 90.90000000000002, "Episode length": 91, "Policy Loss": 3.083411693572998, "Value Loss": 104.78642272949219, "_runtime": 9701.801731348038, "_timestamp": 1585518780.222461, "_step": 202}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": 7.862525939941406, "Value Loss": 62.01352310180664, "_runtime": 9701.939432382584, "_timestamp": 1585518780.360162, "_step": 203}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 0.8294256329536438, "Value Loss": 113.50306701660156, "_runtime": 9702.069584846497, "_timestamp": 1585518780.4903145, "_step": 204}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": 1.2677384614944458, "Value Loss": 115.82568359375, "_runtime": 9702.301546812057, "_timestamp": 1585518780.7222764, "_step": 205}
{"Episode reward": 84.90000000000005, "Episode length": 151, "Policy Loss": 7.459432125091553, "Value Loss": 63.508583068847656, "_runtime": 9702.434573173523, "_timestamp": 1585518780.8553028, "_step": 206}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 1.7299606800079346, "Value Loss": 113.41793060302734, "_runtime": 9702.664064645767, "_timestamp": 1585518781.0847943, "_step": 207}
{"Episode reward": 84.70000000000005, "Episode length": 153, "Policy Loss": 5.287588119506836, "Value Loss": 62.762760162353516, "_runtime": 9702.904872894287, "_timestamp": 1585518781.3256025, "_step": 208}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": 6.402209758758545, "Value Loss": 61.001609802246094, "_runtime": 9703.035300016403, "_timestamp": 1585518781.4560297, "_step": 209}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": 1.0551974773406982, "Value Loss": 115.76795959472656, "_runtime": 9703.274448871613, "_timestamp": 1585518781.6951785, "_step": 210}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": 4.867409706115723, "Value Loss": 61.14628982543945, "_runtime": 9703.417419672012, "_timestamp": 1585518781.8381493, "_step": 211}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.7579484581947327, "Value Loss": 109.19229125976562, "_runtime": 9703.554347038269, "_timestamp": 1585518781.9750767, "_step": 212}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": 0.2511284053325653, "Value Loss": 109.99691009521484, "_runtime": 9703.694386959076, "_timestamp": 1585518782.1151166, "_step": 213}
{"Episode reward": 91.40000000000002, "Episode length": 86, "Policy Loss": 0.990240216255188, "Value Loss": 110.42379760742188, "_runtime": 9703.92483997345, "_timestamp": 1585518782.3455696, "_step": 214}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": 5.075084686279297, "Value Loss": 61.59016799926758, "_runtime": 9704.06359052658, "_timestamp": 1585518782.4843202, "_step": 215}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 1.8768038749694824, "Value Loss": 107.96025848388672, "_runtime": 9704.19495511055, "_timestamp": 1585518782.6156847, "_step": 216}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": 4.186453342437744, "Value Loss": 113.92562866210938, "_runtime": 9704.33329820633, "_timestamp": 1585518782.7540278, "_step": 217}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": 2.8626484870910645, "Value Loss": 110.51986694335938, "_runtime": 9704.470248937607, "_timestamp": 1585518782.8909786, "_step": 218}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": 2.695052146911621, "Value Loss": 109.00306701660156, "_runtime": 9704.713225841522, "_timestamp": 1585518783.1339555, "_step": 219}
{"Episode reward": 83.80000000000004, "Episode length": 162, "Policy Loss": 5.73355770111084, "Value Loss": 59.32404327392578, "_runtime": 9704.853049993515, "_timestamp": 1585518783.2737796, "_step": 220}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": 2.741331100463867, "Value Loss": 106.67455291748047, "_runtime": 9704.990883350372, "_timestamp": 1585518783.411613, "_step": 221}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 2.75044322013855, "Value Loss": 107.69612121582031, "_runtime": 9705.132333040237, "_timestamp": 1585518783.5530627, "_step": 222}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": 3.1615512371063232, "Value Loss": 108.95526885986328, "_runtime": 9705.269758224487, "_timestamp": 1585518783.6904879, "_step": 223}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": 2.4414961338043213, "Value Loss": 109.50079345703125, "_runtime": 9705.408558607101, "_timestamp": 1585518783.8292882, "_step": 224}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 4.23396110534668, "Value Loss": 106.28180694580078, "_runtime": 9705.546060800552, "_timestamp": 1585518783.9667904, "_step": 225}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 1.9578909873962402, "Value Loss": 107.81037902832031, "_runtime": 9705.785729169846, "_timestamp": 1585518784.2064588, "_step": 226}
{"Episode reward": 84.00000000000004, "Episode length": 160, "Policy Loss": 4.677952289581299, "Value Loss": 59.50339889526367, "_runtime": 9705.929479837418, "_timestamp": 1585518784.3502095, "_step": 227}
{"Episode reward": 90.90000000000002, "Episode length": 91, "Policy Loss": -0.5834301710128784, "Value Loss": 105.15750885009766, "_runtime": 9706.074629306793, "_timestamp": 1585518784.495359, "_step": 228}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": -1.803428292274475, "Value Loss": 104.23267364501953, "_runtime": 9706.315141677856, "_timestamp": 1585518784.7358713, "_step": 229}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": 3.9459726810455322, "Value Loss": 60.133609771728516, "_runtime": 9706.557117462158, "_timestamp": 1585518784.977847, "_step": 230}
{"Episode reward": 84.00000000000004, "Episode length": 160, "Policy Loss": 3.6090564727783203, "Value Loss": 59.38118362426758, "_runtime": 9706.690852165222, "_timestamp": 1585518785.1115818, "_step": 231}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": -1.1815770864486694, "Value Loss": 112.02967834472656, "_runtime": 9706.833265304565, "_timestamp": 1585518785.253995, "_step": 232}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 0.2774209678173065, "Value Loss": 106.9725341796875, "_runtime": 9707.076229572296, "_timestamp": 1585518785.4969592, "_step": 233}
{"Episode reward": 84.10000000000005, "Episode length": 159, "Policy Loss": 2.4608142375946045, "Value Loss": 59.582183837890625, "_runtime": 9707.215399980545, "_timestamp": 1585518785.6361296, "_step": 234}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.6898355484008789, "Value Loss": 107.15617370605469, "_runtime": 9707.460010290146, "_timestamp": 1585518785.88074, "_step": 235}
{"Episode reward": 83.70000000000005, "Episode length": 163, "Policy Loss": 1.7078475952148438, "Value Loss": 57.91426467895508, "_runtime": 9707.597450733185, "_timestamp": 1585518786.0181804, "_step": 236}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 0.10462485998868942, "Value Loss": 111.29981994628906, "_runtime": 9707.72684264183, "_timestamp": 1585518786.1475723, "_step": 237}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": 0.05012325942516327, "Value Loss": 113.77961730957031, "_runtime": 9707.869101524353, "_timestamp": 1585518786.2898312, "_step": 238}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": -0.6755297183990479, "Value Loss": 107.82238006591797, "_runtime": 9708.00304722786, "_timestamp": 1585518786.4237769, "_step": 239}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -1.810191035270691, "Value Loss": 109.97755432128906, "_runtime": 9708.14270234108, "_timestamp": 1585518786.563432, "_step": 240}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": -0.15439988672733307, "Value Loss": 106.64087677001953, "_runtime": 9708.3788189888, "_timestamp": 1585518786.7995486, "_step": 241}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": 1.971336007118225, "Value Loss": 59.9857177734375, "_runtime": 9708.618212938309, "_timestamp": 1585518787.0389426, "_step": 242}
{"Episode reward": 84.10000000000005, "Episode length": 159, "Policy Loss": 1.9445892572402954, "Value Loss": 59.4326057434082, "_runtime": 9708.755440235138, "_timestamp": 1585518787.1761699, "_step": 243}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": 1.501109004020691, "Value Loss": 107.41825103759766, "_runtime": 9708.895484924316, "_timestamp": 1585518787.3162146, "_step": 244}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": 1.8153157234191895, "Value Loss": 106.41883087158203, "_runtime": 9709.039383172989, "_timestamp": 1585518787.4601128, "_step": 245}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": 1.1672207117080688, "Value Loss": 106.10443878173828, "_runtime": 9709.17935204506, "_timestamp": 1585518787.6000817, "_step": 246}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": 1.5039399862289429, "Value Loss": 105.73153686523438, "_runtime": 9709.420980215073, "_timestamp": 1585518787.8417099, "_step": 247}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": 1.7955822944641113, "Value Loss": 58.6182746887207, "_runtime": 9709.560413360596, "_timestamp": 1585518787.981143, "_step": 248}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": 0.9810358881950378, "Value Loss": 106.39920806884766, "_runtime": 9709.69616651535, "_timestamp": 1585518788.1168962, "_step": 249}
{"Episode reward": 91.40000000000002, "Episode length": 86, "Policy Loss": -2.2715909481048584, "Value Loss": 108.93083190917969, "_runtime": 9709.848788022995, "_timestamp": 1585518788.2695177, "_step": 250}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": -0.32658013701438904, "Value Loss": 98.89505767822266, "_runtime": 9709.99941778183, "_timestamp": 1585518788.4201474, "_step": 251}
{"Episode reward": 90.30000000000003, "Episode length": 97, "Policy Loss": -4.873045444488525, "Value Loss": 99.64295959472656, "_runtime": 9710.259228229523, "_timestamp": 1585518788.6799579, "_step": 252}
{"Episode reward": 82.60000000000002, "Episode length": 174, "Policy Loss": -0.12410828471183777, "Value Loss": 53.87858200073242, "_runtime": 9710.395267248154, "_timestamp": 1585518788.815997, "_step": 253}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -2.123990774154663, "Value Loss": 110.17193603515625, "_runtime": 9710.532956123352, "_timestamp": 1585518788.9536858, "_step": 254}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": -1.109177589416504, "Value Loss": 107.27674102783203, "_runtime": 9710.677612066269, "_timestamp": 1585518789.0983417, "_step": 255}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -1.7723088264465332, "Value Loss": 106.44141387939453, "_runtime": 9710.811862945557, "_timestamp": 1585518789.2325926, "_step": 256}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -0.46427544951438904, "Value Loss": 110.25477600097656, "_runtime": 9710.950528144836, "_timestamp": 1585518789.3712578, "_step": 257}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.3665219247341156, "Value Loss": 106.80876159667969, "_runtime": 9711.095818042755, "_timestamp": 1585518789.5165477, "_step": 258}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 1.048875331878662, "Value Loss": 101.3880386352539, "_runtime": 9711.237028598785, "_timestamp": 1585518789.6577582, "_step": 259}
{"Episode reward": 91.09472212195399, "Episode length": 90, "Policy Loss": -2.2520132064819336, "Value Loss": 104.37841796875, "_runtime": 9711.487000703812, "_timestamp": 1585518789.9077303, "_step": 260}
{"Episode reward": 83.30000000000004, "Episode length": 167, "Policy Loss": -0.5256689190864563, "Value Loss": 56.895408630371094, "_runtime": 9711.736165046692, "_timestamp": 1585518790.1568947, "_step": 261}
{"Episode reward": 83.50000000000004, "Episode length": 165, "Policy Loss": -0.11789698898792267, "Value Loss": 57.33515548706055, "_runtime": 9711.879064559937, "_timestamp": 1585518790.2997942, "_step": 262}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": -1.0660918951034546, "Value Loss": 104.66526794433594, "_runtime": 9712.025831460953, "_timestamp": 1585518790.446561, "_step": 263}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": -2.0538761615753174, "Value Loss": 103.46053314208984, "_runtime": 9712.171269655228, "_timestamp": 1585518790.5919993, "_step": 264}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": -0.46814441680908203, "Value Loss": 105.21832275390625, "_runtime": 9712.316488265991, "_timestamp": 1585518790.737218, "_step": 265}
{"Episode reward": 90.80000000000003, "Episode length": 92, "Policy Loss": -1.6032837629318237, "Value Loss": 102.08243560791016, "_runtime": 9712.465179920197, "_timestamp": 1585518790.8859096, "_step": 266}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": 1.1039962768554688, "Value Loss": 99.5062484741211, "_runtime": 9712.718702793121, "_timestamp": 1585518791.1394324, "_step": 267}
{"Episode reward": 83.19639686197046, "Episode length": 169, "Policy Loss": 0.37469133734703064, "Value Loss": 56.90255355834961, "_runtime": 9712.975334882736, "_timestamp": 1585518791.3960645, "_step": 268}
{"Episode reward": 83.00000000000003, "Episode length": 170, "Policy Loss": -0.19693617522716522, "Value Loss": 56.300941467285156, "_runtime": 9713.215869188309, "_timestamp": 1585518791.6365988, "_step": 269}
{"Episode reward": 84.10000000000005, "Episode length": 159, "Policy Loss": 0.3408673405647278, "Value Loss": 59.162715911865234, "_runtime": 9713.364430189133, "_timestamp": 1585518791.7851598, "_step": 270}
{"Episode reward": 90.90000000000002, "Episode length": 91, "Policy Loss": 2.7845122814178467, "Value Loss": 103.63642883300781, "_runtime": 9713.61831998825, "_timestamp": 1585518792.0390496, "_step": 271}
{"Episode reward": 83.10000000000004, "Episode length": 169, "Policy Loss": -0.6499946713447571, "Value Loss": 56.656978607177734, "_runtime": 9713.76480126381, "_timestamp": 1585518792.185531, "_step": 272}
{"Episode reward": 90.90000000000002, "Episode length": 91, "Policy Loss": -1.5313016176223755, "Value Loss": 103.4782485961914, "_runtime": 9714.001032829285, "_timestamp": 1585518792.4217625, "_step": 273}
{"Episode reward": 84.20000000000005, "Episode length": 158, "Policy Loss": -0.38873347640037537, "Value Loss": 59.67280197143555, "_runtime": 9714.255546808243, "_timestamp": 1585518792.6762764, "_step": 274}
{"Episode reward": 83.20000000000005, "Episode length": 168, "Policy Loss": -2.107816219329834, "Value Loss": 56.387908935546875, "_runtime": 9714.3928232193, "_timestamp": 1585518792.8135529, "_step": 275}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.21221670508384705, "Value Loss": 107.16724395751953, "_runtime": 9714.534413814545, "_timestamp": 1585518792.9551435, "_step": 276}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -1.9887199401855469, "Value Loss": 106.30944061279297, "_runtime": 9714.67788362503, "_timestamp": 1585518793.0986133, "_step": 277}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.15824252367019653, "Value Loss": 107.50939178466797, "_runtime": 9714.804250001907, "_timestamp": 1585518793.2249796, "_step": 278}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -0.9143853187561035, "Value Loss": 116.51756286621094, "_runtime": 9714.943571567535, "_timestamp": 1585518793.3643012, "_step": 279}
{"Episode reward": 91.10000000000002, "Episode length": 89, "Policy Loss": -3.2505688667297363, "Value Loss": 105.9452896118164, "_runtime": 9715.076091051102, "_timestamp": 1585518793.4968207, "_step": 280}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -2.898145914077759, "Value Loss": 109.79509735107422, "_runtime": 9715.206779718399, "_timestamp": 1585518793.6275094, "_step": 281}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 0.6756317019462585, "Value Loss": 112.211181640625, "_runtime": 9715.342318534851, "_timestamp": 1585518793.7630482, "_step": 282}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": -1.4670547246932983, "Value Loss": 108.61951446533203, "_runtime": 9715.583313941956, "_timestamp": 1585518794.0040436, "_step": 283}
{"Episode reward": 83.70000000000005, "Episode length": 163, "Policy Loss": -1.8057434558868408, "Value Loss": 57.94663619995117, "_runtime": 9715.714765787125, "_timestamp": 1585518794.1354954, "_step": 284}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": -2.0220959186553955, "Value Loss": 111.59945678710938, "_runtime": 9715.842312574387, "_timestamp": 1585518794.2630422, "_step": 285}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -0.5944515466690063, "Value Loss": 115.30607604980469, "_runtime": 9715.988116264343, "_timestamp": 1585518794.408846, "_step": 286}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -1.7447679042816162, "Value Loss": 116.28416442871094, "_runtime": 9716.11818599701, "_timestamp": 1585518794.5389156, "_step": 287}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -0.8794937133789062, "Value Loss": 119.0512924194336, "_runtime": 9716.253618717194, "_timestamp": 1585518794.6743484, "_step": 288}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.9681918621063232, "Value Loss": 107.54322052001953, "_runtime": 9716.383556365967, "_timestamp": 1585518794.804286, "_step": 289}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": -2.143601179122925, "Value Loss": 112.04645538330078, "_runtime": 9716.507171154022, "_timestamp": 1585518794.9279008, "_step": 290}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -0.880846381187439, "Value Loss": 117.1750717163086, "_runtime": 9716.75082540512, "_timestamp": 1585518795.171555, "_step": 291}
{"Episode reward": 83.40000000000003, "Episode length": 166, "Policy Loss": -5.331081390380859, "Value Loss": 56.7797966003418, "_runtime": 9716.881716251373, "_timestamp": 1585518795.302446, "_step": 292}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": 2.2761597633361816, "Value Loss": 111.1125717163086, "_runtime": 9717.004561424255, "_timestamp": 1585518795.425291, "_step": 293}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -2.9065563678741455, "Value Loss": 116.66322326660156, "_runtime": 9717.133877277374, "_timestamp": 1585518795.554607, "_step": 294}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -2.753047466278076, "Value Loss": 117.94523620605469, "_runtime": 9717.361064195633, "_timestamp": 1585518795.7817938, "_step": 295}
{"Episode reward": 84.70000000000005, "Episode length": 153, "Policy Loss": -2.397209405899048, "Value Loss": 61.57484436035156, "_runtime": 9717.492194414139, "_timestamp": 1585518795.912924, "_step": 296}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": -1.3438669443130493, "Value Loss": 113.39805603027344, "_runtime": 9717.614213228226, "_timestamp": 1585518796.0349429, "_step": 297}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -0.6321350932121277, "Value Loss": 119.99868774414062, "gradients/fc.2.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [-0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024, -0.00024558266159147024]}, "gradients/fc.2.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 4.0, 2.0], "bins": [-1.2898480892181396, -1.2691385746002197, -1.2484290599822998, -1.2277195453643799, -1.2070101499557495, -1.1863006353378296, -1.1655911207199097, -1.1448816061019897, -1.1241720914840698, -1.10346257686615, -1.08275306224823, -1.0620436668395996, -1.0413341522216797, -1.0206246376037598, -0.9999151229858398, -0.9792056083679199, -0.95849609375, -0.9377866387367249, -0.9170771241188049, -0.8963676691055298, -0.8756581544876099, -0.8549486398696899, -0.83423912525177, -0.8135296106338501, -0.792820155620575, -0.772110641002655, -0.7514011263847351, -0.73069167137146, -0.70998215675354, -0.6892726421356201, -0.6685631275177002, -0.647853672504425, -0.6271441578865051, -0.6064346432685852, -0.5857251882553101, -0.5650156736373901, -0.5443061590194702, -0.5235966444015503, -0.5028871893882751, -0.4821776747703552, -0.4614681601524353, -0.44075870513916016, -0.42004919052124023, -0.3993396759033203, -0.3786301612854004, -0.35792070627212524, -0.3372111916542053, -0.3165016770362854, -0.29579222202301025, -0.27508270740509033, -0.2543731927871704, -0.2336636781692505, -0.21295416355133057, -0.19224464893341064, -0.17153525352478027, -0.15082573890686035, -0.13011622428894043, -0.10940670967102051, -0.08869719505310059, -0.06798768043518066, -0.04727816581726074, -0.02656877040863037, -0.005859255790710449, 0.014850258827209473, 0.035559773445129395]}, "gradients/fc.1.bias": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], "bins": [-0.009261117316782475, -0.009025529026985168, -0.008789941668510437, -0.008554353378713131, -0.008318765088915825, -0.008083177730441093, -0.007847589440643787, -0.007612001150846481, -0.007376413326710463, -0.007140825502574444, -0.006905237212777138, -0.006669649388641119, -0.0064340615645051, -0.006198473274707794, -0.0059628854505717754, -0.005727297626435757, -0.005491709336638451, -0.005256121512502432, -0.005020533222705126, -0.004784945398569107, -0.004549357108771801, -0.004313769284635782, -0.0040781814604997635, -0.0038425931707024574, -0.0036070053465664387, -0.00337141752243042, -0.003135829232633114, -0.002900241408497095, -0.0026646535843610764, -0.0024290652945637703, -0.0021934774704277515, -0.0019578891806304455, -0.0017223013564944267, -0.001486713532358408, -0.0012511257082223892, -0.0010155374184250832, -0.0007799491286277771, -0.0005443617701530457, -0.0003087734803557396, -7.318519055843353e-05, 0.00016240309923887253, 0.000397990457713604, 0.00063357874751091, 0.0008691670373082161, 0.0011047543957829475, 0.0013403426855802536, 0.0015759309753775597, 0.001811518333852291, 0.002047106623649597, 0.0022826949134469032, 0.0025182822719216347, 0.0027538705617189407, 0.002989458851516247, 0.0032250462099909782, 0.0034606344997882843, 0.0036962227895855904, 0.003931810148060322, 0.004167398437857628, 0.004402986727654934, 0.00463857501745224, 0.0048741623759269714, 0.0051097506657242775, 0.0053453389555215836, 0.005580926313996315, 0.005816514603793621]}, "gradients/fc.1.weight": {"_type": "histogram", "values": [2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 9.0, 13.0, 13.0, 10.0, 6.0, 2.0, 3.0, 5.0, 6.0, 9.0, 7.0, 5.0, 3.0, 4.0, 1.0, 10.0, 6.0, 1.0, 7.0, 18.0, 11.0, 127.0, 27.0, 10.0, 38.0, 29.0, 30.0, 6.0, 7.0, 10.0, 7.0, 9.0, 7.0, 6.0, 4.0, 6.0, 2.0, 0.0, 3.0, 3.0, 4.0, 4.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], "bins": [-0.06715518981218338, -0.06509280949831009, -0.0630304291844368, -0.06096804887056351, -0.058905668556690216, -0.056843288242816925, -0.054780907928943634, -0.05271852761507034, -0.05065614730119705, -0.04859376698732376, -0.04653138667345047, -0.04446900635957718, -0.042406629770994186, -0.040344249457120895, -0.038281869143247604, -0.03621948882937431, -0.03415710851550102, -0.03209472820162773, -0.03003234788775444, -0.02796996757388115, -0.02590758726000786, -0.023845206946134567, -0.021782826632261276, -0.019720446318387985, -0.017658069729804993, -0.015595689415931702, -0.01353330910205841, -0.01147092878818512, -0.009408548474311829, -0.007346168160438538, -0.005283787846565247, -0.0032214075326919556, -0.0011590272188186646, 0.0009033530950546265, 0.0029657334089279175, 0.0050281137228012085, 0.0070904940366744995, 0.00915287435054779, 0.011215254664421082, 0.013277634978294373, 0.015340015292167664, 0.017402395606040955, 0.019464775919914246, 0.021527156233787537, 0.023589536547660828, 0.02565191686153412, 0.02771429717540741, 0.0297766774892807, 0.031839050352573395, 0.033901430666446686, 0.03596381098031998, 0.03802619129419327, 0.04008857160806656, 0.04215095192193985, 0.04421333223581314, 0.04627571254968643, 0.04833809286355972, 0.050400473177433014, 0.052462853491306305, 0.054525233805179596, 0.05658761411905289, 0.05864999443292618, 0.06071237474679947, 0.06277475506067276, 0.06483713537454605]}, "gradients/fc.0.bias": {"_type": "histogram", "values": [2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 5.0, 4.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 0.0, 3.0], "bins": [-0.3295170068740845, -0.3209099769592285, -0.31230297684669495, -0.303695946931839, -0.2950889468193054, -0.28648191690444946, -0.2778748869895935, -0.26926788687705994, -0.260660856962204, -0.2520538568496704, -0.24344682693481445, -0.2348398119211197, -0.22623279690742493, -0.21762576699256897, -0.2090187668800354, -0.20041173696517944, -0.19180472195148468, -0.18319770693778992, -0.17459069192409515, -0.1659836620092392, -0.15737664699554443, -0.14876963198184967, -0.1401626169681549, -0.13155560195446014, -0.12294858694076538, -0.11434155702590942, -0.10573454201221466, -0.0971275269985199, -0.08852051198482513, -0.07991349697113037, -0.07130646705627441, -0.06269946694374084, -0.05409243702888489, -0.04548540711402893, -0.03687840700149536, -0.028271377086639404, -0.019664376974105835, -0.011057347059249878, -0.002450317144393921, 0.0061566829681396484, 0.014763712882995605, 0.023370712995529175, 0.03197774291038513, 0.04058477282524109, 0.04919177293777466, 0.057798802852630615, 0.06640580296516418, 0.07501283288002014, 0.08361983299255371, 0.09222686290740967, 0.10083389282226562, 0.1094408929347992, 0.11804792284965515, 0.12665492296218872, 0.13526195287704468, 0.14386898279190063, 0.1524759829044342, 0.16108301281929016, 0.16969001293182373, 0.1782970428466797, 0.18690407276153564, 0.1955111026763916, 0.20411807298660278, 0.21272510290145874, 0.2213321328163147]}, "gradients/fc.0.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 5.0, 5.0, 3.0, 2.0, 8.0, 5.0, 3.0, 5.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0], "bins": [-0.19510279595851898, -0.18713660538196564, -0.1791704148054123, -0.17120423913002014, -0.1632380485534668, -0.15527185797691345, -0.1473056674003601, -0.13933947682380676, -0.13137328624725342, -0.12340710312128067, -0.11544091999530792, -0.10747472941875458, -0.09950853884220123, -0.09154235571622849, -0.08357616513967514, -0.07560998201370239, -0.06764379143714905, -0.0596776008605957, -0.05171141028404236, -0.04374523460865021, -0.03577904403209686, -0.027812853455543518, -0.019846662878990173, -0.011880472302436829, -0.003914281725883484, 0.004051893949508667, 0.012018084526062012, 0.019984275102615356, 0.0279504656791687, 0.035916656255722046, 0.0438828319311142, 0.05184902250766754, 0.059815213084220886, 0.06778140366077423, 0.07574759423732758, 0.08371378481388092, 0.09167997539043427, 0.09964616596698761, 0.10761232674121857, 0.11557851731777191, 0.12354470789432526, 0.1315108984708786, 0.13947708904743195, 0.1474432796239853, 0.15540947020053864, 0.16337566077709198, 0.17134185135364532, 0.17930804193019867, 0.18727423250675201, 0.19524039328098297, 0.20320658385753632, 0.21117277443408966, 0.219138965010643, 0.22710515558719635, 0.2350713461637497, 0.24303753674030304, 0.2510037422180176, 0.25896990299224854, 0.26693612337112427, 0.2749022841453552, 0.2828684449195862, 0.2908346652984619, 0.29880082607269287, 0.3067670464515686, 0.31473320722579956]}, "_runtime": 9717.743024349213, "_timestamp": 1585518796.163754, "_step": 298}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -0.7281615138053894, "Value Loss": 118.77632141113281, "_runtime": 9717.972243309021, "_timestamp": 1585518796.392973, "_step": 299}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": -4.233060836791992, "Value Loss": 60.93448257446289, "_runtime": 9718.123772859573, "_timestamp": 1585518796.5445025, "_step": 300}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": -2.445885419845581, "Value Loss": 113.36455535888672, "_runtime": 9718.250187158585, "_timestamp": 1585518796.6709168, "_step": 301}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -4.514678001403809, "Value Loss": 118.38548278808594, "_runtime": 9718.46854352951, "_timestamp": 1585518796.8892732, "_step": 302}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -3.9971821308135986, "Value Loss": 65.15786743164062, "_runtime": 9718.5929646492, "_timestamp": 1585518797.0136943, "_step": 303}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -4.209134101867676, "Value Loss": 116.55001068115234, "_runtime": 9718.722491264343, "_timestamp": 1585518797.143221, "_step": 304}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": -5.500782012939453, "Value Loss": 113.0693588256836, "_runtime": 9718.854543685913, "_timestamp": 1585518797.2752733, "_step": 305}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": -2.8070671558380127, "Value Loss": 114.54981994628906, "_runtime": 9718.977449655533, "_timestamp": 1585518797.3981793, "_step": 306}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -3.530787706375122, "Value Loss": 118.051025390625, "_runtime": 9719.102583408356, "_timestamp": 1585518797.523313, "_step": 307}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -1.1726040840148926, "Value Loss": 115.83784484863281, "_runtime": 9719.224354028702, "_timestamp": 1585518797.6450837, "_step": 308}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -4.130148410797119, "Value Loss": 118.99504089355469, "_runtime": 9719.350865840912, "_timestamp": 1585518797.7715955, "_step": 309}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -3.3899550437927246, "Value Loss": 115.41975402832031, "_runtime": 9719.481729745865, "_timestamp": 1585518797.9024594, "_step": 310}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -5.582675457000732, "Value Loss": 110.11734771728516, "_runtime": 9719.606452465057, "_timestamp": 1585518798.027182, "_step": 311}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": 1.6614208221435547, "Value Loss": 117.24296569824219, "_runtime": 9719.73142695427, "_timestamp": 1585518798.1521566, "_step": 312}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -3.4938950538635254, "Value Loss": 114.98969268798828, "_runtime": 9719.854577541351, "_timestamp": 1585518798.2753072, "_step": 313}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": 0.9832231402397156, "Value Loss": 120.05004119873047, "_runtime": 9719.979120254517, "_timestamp": 1585518798.39985, "_step": 314}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -2.352247476577759, "Value Loss": 118.04344940185547, "_runtime": 9720.210195302963, "_timestamp": 1585518798.630925, "_step": 315}
{"Episode reward": 84.50000000000004, "Episode length": 155, "Policy Loss": -3.824126958847046, "Value Loss": 61.027835845947266, "_runtime": 9720.338016271591, "_timestamp": 1585518798.758746, "_step": 316}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -4.117127895355225, "Value Loss": 115.19961547851562, "_runtime": 9720.552563428879, "_timestamp": 1585518798.973293, "_step": 317}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -3.2849433422088623, "Value Loss": 65.34060668945312, "_runtime": 9720.682153224945, "_timestamp": 1585518799.1028829, "_step": 318}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -3.415355920791626, "Value Loss": 116.6712417602539, "_runtime": 9720.806930065155, "_timestamp": 1585518799.2276597, "_step": 319}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": 0.6246708035469055, "Value Loss": 118.46293640136719, "_runtime": 9721.04195690155, "_timestamp": 1585518799.4626865, "_step": 320}
{"Episode reward": 84.30000000000004, "Episode length": 157, "Policy Loss": -4.970753192901611, "Value Loss": 60.173553466796875, "_runtime": 9721.16931271553, "_timestamp": 1585518799.5900424, "_step": 321}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -4.146422386169434, "Value Loss": 114.21752166748047, "_runtime": 9721.291861057281, "_timestamp": 1585518799.7125907, "_step": 322}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -0.38174888491630554, "Value Loss": 119.7457275390625, "_runtime": 9721.421996831894, "_timestamp": 1585518799.8427265, "_step": 323}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -3.4675545692443848, "Value Loss": 116.32698059082031, "_runtime": 9721.544361829758, "_timestamp": 1585518799.9650915, "_step": 324}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -0.4899614155292511, "Value Loss": 121.12157440185547, "_runtime": 9721.767114162445, "_timestamp": 1585518800.1878438, "_step": 325}
{"Episode reward": 85.10000000000004, "Episode length": 149, "Policy Loss": -3.538574695587158, "Value Loss": 63.1350212097168, "_runtime": 9721.889228343964, "_timestamp": 1585518800.309958, "_step": 326}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.6163628101348877, "Value Loss": 121.38801574707031, "_runtime": 9722.014029741287, "_timestamp": 1585518800.4347594, "_step": 327}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -2.592982053756714, "Value Loss": 117.16143798828125, "_runtime": 9722.139297246933, "_timestamp": 1585518800.560027, "_step": 328}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": 3.0268337726593018, "Value Loss": 121.86570739746094, "_runtime": 9722.342541694641, "_timestamp": 1585518800.7632713, "_step": 329}
{"Episode reward": 86.50000000000004, "Episode length": 135, "Policy Loss": -3.889833688735962, "Value Loss": 69.45916748046875, "_runtime": 9722.461360692978, "_timestamp": 1585518800.8820903, "_step": 330}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -5.866694927215576, "Value Loss": 123.94332122802734, "_runtime": 9722.577080488205, "_timestamp": 1585518800.9978101, "_step": 331}
{"Episode reward": 92.70000000000002, "Episode length": 73, "Policy Loss": -3.0393757820129395, "Value Loss": 127.7184829711914, "_runtime": 9722.700437784195, "_timestamp": 1585518801.1211674, "_step": 332}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -2.7821714878082275, "Value Loss": 123.19627380371094, "_runtime": 9722.821442127228, "_timestamp": 1585518801.2421718, "_step": 333}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.3145346641540527, "Value Loss": 121.59639739990234, "_runtime": 9722.93530535698, "_timestamp": 1585518801.356035, "_step": 334}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -0.6343761682510376, "Value Loss": 129.2012939453125, "_runtime": 9723.05593252182, "_timestamp": 1585518801.4766622, "_step": 335}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.9672389030456543, "Value Loss": 123.16700744628906, "_runtime": 9723.214368104935, "_timestamp": 1585518801.6350977, "_step": 336}
{"Episode reward": 89.70000000000003, "Episode length": 103, "Policy Loss": 9.376522064208984, "Value Loss": 91.1247329711914, "_runtime": 9723.365832567215, "_timestamp": 1585518801.7865622, "_step": 337}
{"Episode reward": 90.20000000000002, "Episode length": 98, "Policy Loss": 6.817030906677246, "Value Loss": 94.6711654663086, "_runtime": 9723.480974912643, "_timestamp": 1585518801.9017045, "_step": 338}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -4.117188453674316, "Value Loss": 129.08322143554688, "_runtime": 9723.707998514175, "_timestamp": 1585518802.1287282, "_step": 339}
{"Episode reward": 84.90000000000005, "Episode length": 151, "Policy Loss": -4.984985828399658, "Value Loss": 62.37853240966797, "_runtime": 9723.931871414185, "_timestamp": 1585518802.352601, "_step": 340}
{"Episode reward": 85.00000000000004, "Episode length": 150, "Policy Loss": -2.0552289485931396, "Value Loss": 63.3593864440918, "_runtime": 9724.049809217453, "_timestamp": 1585518802.4705389, "_step": 341}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -2.308128833770752, "Value Loss": 124.84896087646484, "_runtime": 9724.16833615303, "_timestamp": 1585518802.5890658, "_step": 342}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -2.521657943725586, "Value Loss": 129.2519989013672, "_runtime": 9724.327268600464, "_timestamp": 1585518802.7479982, "_step": 343}
{"Episode reward": 89.90000000000002, "Episode length": 101, "Policy Loss": 6.501591205596924, "Value Loss": 92.22714233398438, "_runtime": 9724.444439172745, "_timestamp": 1585518802.8651688, "_step": 344}
{"Episode reward": 92.60000000000002, "Episode length": 74, "Policy Loss": 1.2127659320831299, "Value Loss": 126.78862762451172, "_runtime": 9724.589234352112, "_timestamp": 1585518803.009964, "_step": 345}
{"Episode reward": 90.60000000000002, "Episode length": 94, "Policy Loss": 5.786344051361084, "Value Loss": 98.3263168334961, "_runtime": 9724.709194660187, "_timestamp": 1585518803.1299243, "_step": 346}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -1.10050368309021, "Value Loss": 124.90374755859375, "_runtime": 9724.829715251923, "_timestamp": 1585518803.250445, "_step": 347}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -1.5873045921325684, "Value Loss": 121.80516052246094, "_runtime": 9724.987089157104, "_timestamp": 1585518803.4078188, "_step": 348}
{"Episode reward": 89.80000000000003, "Episode length": 102, "Policy Loss": 5.516451358795166, "Value Loss": 91.9249496459961, "_runtime": 9725.19774222374, "_timestamp": 1585518803.6184719, "_step": 349}
{"Episode reward": 85.99855718612675, "Episode length": 141, "Policy Loss": -4.863458156585693, "Value Loss": 65.92298889160156, "_runtime": 9725.347689390182, "_timestamp": 1585518803.768419, "_step": 350}
{"Episode reward": 90.20000000000002, "Episode length": 98, "Policy Loss": 8.209416389465332, "Value Loss": 94.53012084960938, "_runtime": 9725.462537765503, "_timestamp": 1585518803.8832674, "_step": 351}
{"Episode reward": 92.90000000000002, "Episode length": 71, "Policy Loss": 0.8941201567649841, "Value Loss": 131.41114807128906, "_runtime": 9725.59086894989, "_timestamp": 1585518804.0115986, "_step": 352}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -0.6382696628570557, "Value Loss": 121.06006622314453, "_runtime": 9725.749515771866, "_timestamp": 1585518804.1702454, "_step": 353}
{"Episode reward": 89.70000000000003, "Episode length": 103, "Policy Loss": 6.1046881675720215, "Value Loss": 90.40940856933594, "_runtime": 9725.903671503067, "_timestamp": 1585518804.3244011, "_step": 354}
{"Episode reward": 89.90000000000002, "Episode length": 101, "Policy Loss": 5.357453346252441, "Value Loss": 91.78764343261719, "_runtime": 9726.05786061287, "_timestamp": 1585518804.4785903, "_step": 355}
{"Episode reward": 89.90000000000002, "Episode length": 101, "Policy Loss": 4.476802825927734, "Value Loss": 92.10936737060547, "_runtime": 9726.213728189468, "_timestamp": 1585518804.6344578, "_step": 356}
{"Episode reward": 89.90000000000002, "Episode length": 101, "Policy Loss": 6.483086585998535, "Value Loss": 91.84220123291016, "_runtime": 9726.433628082275, "_timestamp": 1585518804.8543577, "_step": 357}
{"Episode reward": 85.30000000000004, "Episode length": 147, "Policy Loss": -6.363908290863037, "Value Loss": 63.481685638427734, "_runtime": 9726.5500061512, "_timestamp": 1585518804.9707358, "_step": 358}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -2.1910810470581055, "Value Loss": 128.7116241455078, "_runtime": 9726.7098300457, "_timestamp": 1585518805.1305597, "_step": 359}
{"Episode reward": 89.60000000000002, "Episode length": 104, "Policy Loss": 5.996246814727783, "Value Loss": 89.65996551513672, "_runtime": 9726.860611915588, "_timestamp": 1585518805.2813416, "_step": 360}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": 5.724750518798828, "Value Loss": 98.7549057006836, "_runtime": 9727.018435001373, "_timestamp": 1585518805.4391646, "_step": 361}
{"Episode reward": 89.60000000000002, "Episode length": 104, "Policy Loss": 4.091475009918213, "Value Loss": 89.13430786132812, "_runtime": 9727.240046977997, "_timestamp": 1585518805.6607766, "_step": 362}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -6.810894966125488, "Value Loss": 63.49919509887695, "_runtime": 9727.448391199112, "_timestamp": 1585518805.8691208, "_step": 363}
{"Episode reward": 86.20000000000005, "Episode length": 138, "Policy Loss": -5.558888912200928, "Value Loss": 67.28672790527344, "_runtime": 9727.564674854279, "_timestamp": 1585518805.9854045, "_step": 364}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -2.280850648880005, "Value Loss": 128.2830352783203, "_runtime": 9727.784002304077, "_timestamp": 1585518806.204732, "_step": 365}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": -3.96492338180542, "Value Loss": 64.41915893554688, "_runtime": 9727.906345129013, "_timestamp": 1585518806.3270748, "_step": 366}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -1.9173171520233154, "Value Loss": 123.74885559082031, "_runtime": 9728.024419546127, "_timestamp": 1585518806.4451492, "_step": 367}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -0.2195291668176651, "Value Loss": 124.31519317626953, "_runtime": 9728.151804924011, "_timestamp": 1585518806.5725346, "_step": 368}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -0.2648477852344513, "Value Loss": 119.85126495361328, "_runtime": 9728.270187139511, "_timestamp": 1585518806.6909168, "_step": 369}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": 0.6330311298370361, "Value Loss": 124.20941162109375, "_runtime": 9728.391733169556, "_timestamp": 1585518806.8124628, "_step": 370}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -1.0651068687438965, "Value Loss": 121.74657440185547, "_runtime": 9728.511907339096, "_timestamp": 1585518806.932637, "_step": 371}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -3.8603270053863525, "Value Loss": 120.86630249023438, "_runtime": 9728.637141942978, "_timestamp": 1585518807.0578716, "_step": 372}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -2.1169991493225098, "Value Loss": 116.35504150390625, "_runtime": 9728.756580591202, "_timestamp": 1585518807.1773102, "_step": 373}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -0.18931128084659576, "Value Loss": 122.42378997802734, "_runtime": 9728.871535539627, "_timestamp": 1585518807.2922652, "_step": 374}
{"Episode reward": 92.80000000000001, "Episode length": 72, "Policy Loss": -1.3588567972183228, "Value Loss": 127.72246551513672, "_runtime": 9728.990325689316, "_timestamp": 1585518807.4110553, "_step": 375}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -2.54286527633667, "Value Loss": 123.45653533935547, "_runtime": 9729.10619544983, "_timestamp": 1585518807.526925, "_step": 376}
{"Episode reward": 92.70000000000002, "Episode length": 73, "Policy Loss": -2.016019821166992, "Value Loss": 126.07809448242188, "_runtime": 9729.325717926025, "_timestamp": 1585518807.7464476, "_step": 377}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -6.683460235595703, "Value Loss": 63.32379150390625, "_runtime": 9729.455094575882, "_timestamp": 1585518807.8758242, "_step": 378}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": -2.709348201751709, "Value Loss": 112.8810806274414, "_runtime": 9729.669043064117, "_timestamp": 1585518808.0897727, "_step": 379}
{"Episode reward": 85.60000000000004, "Episode length": 144, "Policy Loss": -6.680874824523926, "Value Loss": 65.19092559814453, "_runtime": 9729.790533304214, "_timestamp": 1585518808.211263, "_step": 380}
{"Episode reward": 92.60000000000002, "Episode length": 74, "Policy Loss": -3.5682520866394043, "Value Loss": 123.69058990478516, "_runtime": 9730.02488207817, "_timestamp": 1585518808.4456117, "_step": 381}
{"Episode reward": 84.20000000000005, "Episode length": 158, "Policy Loss": -7.844168663024902, "Value Loss": 59.881858825683594, "_runtime": 9730.150599479675, "_timestamp": 1585518808.571329, "_step": 382}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.84867525100708, "Value Loss": 120.04271697998047, "_runtime": 9730.385539770126, "_timestamp": 1585518808.8062694, "_step": 383}
{"Episode reward": 84.20000000000005, "Episode length": 158, "Policy Loss": -5.830002307891846, "Value Loss": 59.62742233276367, "_runtime": 9730.51472067833, "_timestamp": 1585518808.9354503, "_step": 384}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -2.6229281425476074, "Value Loss": 119.01646423339844, "_runtime": 9730.732478141785, "_timestamp": 1585518809.1532078, "_step": 385}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -3.5303001403808594, "Value Loss": 64.1655044555664, "_runtime": 9730.942029476166, "_timestamp": 1585518809.362759, "_step": 386}
{"Episode reward": 86.30000000000004, "Episode length": 137, "Policy Loss": -3.634004592895508, "Value Loss": 68.16693115234375, "_runtime": 9731.169143915176, "_timestamp": 1585518809.5898736, "_step": 387}
{"Episode reward": 84.90000000000005, "Episode length": 151, "Policy Loss": -2.1420326232910156, "Value Loss": 62.21121597290039, "_runtime": 9731.382889270782, "_timestamp": 1585518809.803619, "_step": 388}
{"Episode reward": 86.00000000000004, "Episode length": 140, "Policy Loss": -4.072082996368408, "Value Loss": 66.37691497802734, "_runtime": 9731.511376857758, "_timestamp": 1585518809.9321065, "_step": 389}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -2.0660600662231445, "Value Loss": 117.4169921875, "_runtime": 9731.64854812622, "_timestamp": 1585518810.0692778, "_step": 390}
{"Episode reward": 91.50000000000001, "Episode length": 85, "Policy Loss": -3.6967217922210693, "Value Loss": 112.66969299316406, "_runtime": 9731.775985717773, "_timestamp": 1585518810.1967154, "_step": 391}
{"Episode reward": 92.10000000000002, "Episode length": 79, "Policy Loss": -2.798279047012329, "Value Loss": 117.72502899169922, "_runtime": 9731.895370006561, "_timestamp": 1585518810.3160996, "_step": 392}
{"Episode reward": 92.50000000000001, "Episode length": 75, "Policy Loss": -5.786930561065674, "Value Loss": 122.52920532226562, "_runtime": 9732.018347263336, "_timestamp": 1585518810.439077, "_step": 393}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -0.2287532538175583, "Value Loss": 119.40498352050781, "_runtime": 9732.140558242798, "_timestamp": 1585518810.5612879, "_step": 394}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": -2.0716748237609863, "Value Loss": 120.66969299316406, "_runtime": 9732.359619617462, "_timestamp": 1585518810.7803493, "_step": 395}
{"Episode reward": 85.30000000000004, "Episode length": 147, "Policy Loss": -1.53281569480896, "Value Loss": 63.95271301269531, "_runtime": 9732.48093032837, "_timestamp": 1585518810.90166, "_step": 396}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.8504269123077393, "Value Loss": 120.80979919433594, "_runtime": 9732.698542356491, "_timestamp": 1585518811.119272, "_step": 397}
{"Episode reward": 85.40000000000003, "Episode length": 146, "Policy Loss": -3.204461097717285, "Value Loss": 64.28084564208984, "gradients/fc.2.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [-0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951, -0.02751193381845951]}, "gradients/fc.2.weight": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0], "bins": [-13.109882354736328, -12.904397964477539, -12.69891357421875, -12.493429183959961, -12.287944793701172, -12.082460403442383, -11.876976013183594, -11.671491622924805, -11.466008186340332, -11.260523796081543, -11.055039405822754, -10.849555015563965, -10.644070625305176, -10.438586235046387, -10.233101844787598, -10.027617454528809, -9.82213306427002, -9.61664867401123, -9.411165237426758, -9.205680847167969, -9.00019645690918, -8.79471206665039, -8.589227676391602, -8.383743286132812, -8.178258895874023, -7.972774505615234, -7.767290115356445, -7.561805725097656, -7.356321334838867, -7.150837421417236, -6.945353031158447, -6.739868640899658, -6.534384250640869, -6.32889986038208, -6.123415470123291, -5.917931079864502, -5.712447166442871, -5.506962776184082, -5.301478385925293, -5.095993995666504, -4.890509605407715, -4.685025215148926, -4.479540824890137, -4.274056434631348, -4.068572044372559, -3.863088607788086, -3.657604217529297, -3.452119827270508, -3.2466354370117188, -3.0411510467529297, -2.8356666564941406, -2.6301822662353516, -2.4246978759765625, -2.2192134857177734, -2.0137290954589844, -1.8082447052001953, -1.6027603149414062, -1.3972759246826172, -1.1917924880981445, -0.9863080978393555, -0.7808237075805664, -0.5753393173217773, -0.3698549270629883, -0.16437053680419922, 0.041113853454589844]}, "gradients/fc.1.bias": {"_type": "histogram", "values": [2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], "bins": [-0.05016108229756355, -0.048826493322849274, -0.04749190807342529, -0.04615732282400131, -0.04482273384928703, -0.043488144874572754, -0.04215355962514877, -0.04081897437572479, -0.03948438540101051, -0.038149796426296234, -0.03681521117687225, -0.03548062592744827, -0.034146036952733994, -0.032811447978019714, -0.031476862728595734, -0.030142275616526604, -0.028807688504457474, -0.027473101392388344, -0.026138514280319214, -0.024803927168250084, -0.023469340056180954, -0.022134752944111824, -0.020800165832042694, -0.019465578719973564, -0.018130991607904434, -0.016796406358480453, -0.015461817383766174, -0.014127228409051895, -0.012792643159627914, -0.011458057910203934, -0.010123468935489655, -0.008788879960775375, -0.007454294711351395, -0.006119709461927414, -0.004785120487213135, -0.0034505315124988556, -0.002115946263074875, -0.0007813610136508942, 0.000553227961063385, 0.0018878169357776642, 0.003222402185201645, 0.004556987434625626, 0.005891576409339905, 0.007226165384054184, 0.008560750633478165, 0.009895335882902145, 0.011229924857616425, 0.012564513832330704, 0.013899099081754684, 0.015233684331178665, 0.016568269580602646, 0.017902862280607224, 0.019237447530031204, 0.020572032779455185, 0.021906625479459763, 0.023241210728883743, 0.024575795978307724, 0.025910381227731705, 0.027244966477155685, 0.028579559177160263, 0.029914144426584244, 0.031248729676008224, 0.0325833223760128, 0.03391790762543678, 0.035252492874860764]}, "gradients/fc.1.weight": {"_type": "histogram", "values": [2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 6.0, 4.0, 6.0, 7.0, 3.0, 5.0, 4.0, 5.0, 3.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.0, 12.0, 11.0, 12.0, 3.0, 9.0, 11.0, 31.0, 82.0, 171.0, 24.0, 37.0, 3.0, 9.0, 5.0, 4.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 0.0, 4.0, 3.0, 5.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], "bins": [-0.7429458498954773, -0.722468376159668, -0.7019909620285034, -0.6815134882926941, -0.6610360741615295, -0.6405586004257202, -0.6200811862945557, -0.5996037125587463, -0.579126238822937, -0.5586488246917725, -0.5381714105606079, -0.5176939368247986, -0.49721646308898926, -0.4767390191555023, -0.4562615752220154, -0.43578413128852844, -0.4153066873550415, -0.39482924342155457, -0.3743517994880676, -0.3538743555545807, -0.33339691162109375, -0.3129194378852844, -0.2924419939517975, -0.27196455001831055, -0.2514871060848236, -0.23100966215133667, -0.21053218841552734, -0.1900547742843628, -0.16957730054855347, -0.14909988641738892, -0.1286224126815796, -0.10814499855041504, -0.08766752481460571, -0.06719005107879639, -0.046712636947631836, -0.02623516321182251, -0.005757749080657959, 0.014719724655151367, 0.03519713878631592, 0.055674612522125244, 0.0761520266532898, 0.09662950038909912, 0.11710697412490845, 0.137584388256073, 0.15806186199188232, 0.17853927612304688, 0.1990167498588562, 0.21949416399002075, 0.23997163772583008, 0.2604491114616394, 0.28092652559280396, 0.3014039397239685, 0.3218814730644226, 0.34235888719558716, 0.3628363013267517, 0.38331371545791626, 0.40379124879837036, 0.4242686629295349, 0.44474607706069946, 0.46522361040115356, 0.4857010245323181, 0.5061784386634827, 0.5266558527946472, 0.5471333861351013, 0.5676108002662659]}, "gradients/fc.0.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 3.0, 3.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-0.7578925490379333, -0.7420260310173035, -0.7261595726013184, -0.7102930545806885, -0.6944265365600586, -0.6785600185394287, -0.6626935601234436, -0.6468270421028137, -0.6309605240821838, -0.6150940656661987, -0.5992275476455688, -0.583361029624939, -0.5674945116043091, -0.551628053188324, -0.5357615351676941, -0.5198950171470642, -0.5040285587310791, -0.4881620407104492, -0.47229552268981934, -0.45642903447151184, -0.44056251645088196, -0.42469602823257446, -0.4088295102119446, -0.3929630219936371, -0.3770965337753296, -0.3612300157546997, -0.3453635275363922, -0.32949700951576233, -0.31363052129745483, -0.29776400327682495, -0.28189751505851746, -0.2660309970378876, -0.2501645088195801, -0.2342979907989502, -0.2184315323829651, -0.2025650143623352, -0.18669849634170532, -0.17083197832107544, -0.15496551990509033, -0.13909900188446045, -0.12323248386383057, -0.10736602544784546, -0.09149950742721558, -0.0756329894065857, -0.05976647138595581, -0.0439000129699707, -0.02803349494934082, -0.012166976928710938, 0.00369948148727417, 0.019565999507904053, 0.035432517528533936, 0.05129903554916382, 0.06716549396514893, 0.08303201198577881, 0.09889853000640869, 0.11476504802703857, 0.13063150644302368, 0.14649802446365356, 0.16236454248428345, 0.17823100090026855, 0.19409751892089844, 0.20996403694152832, 0.2258305549621582, 0.2416970133781433, 0.2575635313987732]}, "gradients/fc.0.weight": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 4.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 3.0, 2.0, 0.0, 1.0, 0.0, 8.0, 5.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0], "bins": [-0.7962692379951477, -0.7794989347457886, -0.7627286314964294, -0.7459582686424255, -0.7291879653930664, -0.7124176621437073, -0.6956473588943481, -0.678877055644989, -0.6621067523956299, -0.645336389541626, -0.6285660862922668, -0.6117957830429077, -0.5950254797935486, -0.5782551765441895, -0.5614848136901855, -0.5447145104408264, -0.5279442071914673, -0.5111739039421082, -0.49440357089042664, -0.4776332676410675, -0.460862934589386, -0.44409263134002686, -0.4273223280906677, -0.4105519950389862, -0.3937816917896271, -0.37701138854026794, -0.3602410554885864, -0.3434707522392273, -0.32670044898986816, -0.30993011593818665, -0.2931597828865051, -0.276389479637146, -0.25961917638778687, -0.24284887313842773, -0.2260785698890686, -0.2093082070350647, -0.19253790378570557, -0.17576760053634644, -0.1589972972869873, -0.14222699403762817, -0.12545663118362427, -0.10868632793426514, -0.091916024684906, -0.07514572143554688, -0.058375418186187744, -0.04160511493682861, -0.024834752082824707, -0.008064448833465576, 0.008705854415893555, 0.025476157665252686, 0.042246460914611816, 0.05901682376861572, 0.07578712701797485, 0.09255743026733398, 0.10932773351669312, 0.12609803676605225, 0.14286834001541138, 0.15963870286941528, 0.17640900611877441, 0.19317930936813354, 0.20994967222213745, 0.2267199158668518, 0.2434902787208557, 0.26026052236557007, 0.277030885219574]}, "_runtime": 9732.923278093338, "_timestamp": 1585518811.3440077, "_step": 398}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -6.5384626388549805, "Value Loss": 63.358768463134766, "_runtime": 9733.15823841095, "_timestamp": 1585518811.578968, "_step": 399}
{"Episode reward": 84.40000000000005, "Episode length": 156, "Policy Loss": -6.126754283905029, "Value Loss": 60.23426055908203, "_runtime": 9733.391753196716, "_timestamp": 1585518811.8124828, "_step": 400}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": -3.7709152698516846, "Value Loss": 60.825016021728516, "_runtime": 9733.530064582825, "_timestamp": 1585518811.9507942, "_step": 401}
{"Episode reward": 91.40000000000002, "Episode length": 86, "Policy Loss": -3.380737066268921, "Value Loss": 111.43350219726562, "_runtime": 9733.654871702194, "_timestamp": 1585518812.0756013, "_step": 402}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -1.0524163246154785, "Value Loss": 121.72145080566406, "_runtime": 9733.780174016953, "_timestamp": 1585518812.2009037, "_step": 403}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -3.5390565395355225, "Value Loss": 120.64842224121094, "_runtime": 9733.994172334671, "_timestamp": 1585518812.414902, "_step": 404}
{"Episode reward": 85.70000000000005, "Episode length": 143, "Policy Loss": -5.889345169067383, "Value Loss": 65.44962310791016, "_runtime": 9734.121294260025, "_timestamp": 1585518812.542024, "_step": 405}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -2.0511176586151123, "Value Loss": 114.86193084716797, "_runtime": 9734.34175825119, "_timestamp": 1585518812.762488, "_step": 406}
{"Episode reward": 85.10000000000004, "Episode length": 149, "Policy Loss": -4.169753074645996, "Value Loss": 62.78688049316406, "_runtime": 9734.468938350677, "_timestamp": 1585518812.889668, "_step": 407}
{"Episode reward": 92.20000000000002, "Episode length": 78, "Policy Loss": 0.44364166259765625, "Value Loss": 118.69501495361328, "_runtime": 9734.706186294556, "_timestamp": 1585518813.126916, "_step": 408}
{"Episode reward": 84.00000000000004, "Episode length": 160, "Policy Loss": -1.8791782855987549, "Value Loss": 58.71686553955078, "_runtime": 9734.836535215378, "_timestamp": 1585518813.2572649, "_step": 409}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": -0.8187187314033508, "Value Loss": 115.8773193359375, "_runtime": 9734.962468624115, "_timestamp": 1585518813.3831983, "_step": 410}
{"Episode reward": 92.00000000000001, "Episode length": 80, "Policy Loss": 2.9078972339630127, "Value Loss": 116.8629379272461, "_runtime": 9735.187857866287, "_timestamp": 1585518813.6085875, "_step": 411}
{"Episode reward": 85.20000000000005, "Episode length": 148, "Policy Loss": -5.619077205657959, "Value Loss": 63.42752456665039, "_runtime": 9735.308615207672, "_timestamp": 1585518813.7293448, "_step": 412}
{"Episode reward": 92.40000000000002, "Episode length": 76, "Policy Loss": -0.6426827907562256, "Value Loss": 121.09397888183594, "_runtime": 9735.430030584335, "_timestamp": 1585518813.8507602, "_step": 413}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": 1.2765562534332275, "Value Loss": 120.47254180908203, "_runtime": 9735.555532217026, "_timestamp": 1585518813.9762619, "_step": 414}
{"Episode reward": 92.30000000000001, "Episode length": 77, "Policy Loss": -2.314332962036133, "Value Loss": 119.42234802246094, "_runtime": 9735.771129846573, "_timestamp": 1585518814.1918595, "_step": 415}
{"Episode reward": 85.50000000000004, "Episode length": 145, "Policy Loss": -5.1970672607421875, "Value Loss": 64.25128173828125, "_runtime": 9735.999551534653, "_timestamp": 1585518814.4202812, "_step": 416}
{"Episode reward": 84.60000000000005, "Episode length": 154, "Policy Loss": -8.756443977355957, "Value Loss": 61.508155822753906, "_runtime": 9736.13425064087, "_timestamp": 1585518814.5549803, "_step": 417}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 1.1918951272964478, "Value Loss": 111.58546447753906, "_runtime": 9736.265032052994, "_timestamp": 1585518814.6857617, "_step": 418}
{"Episode reward": 91.90000000000002, "Episode length": 81, "Policy Loss": -2.8239078521728516, "Value Loss": 113.46208190917969, "_runtime": 9736.399817705154, "_timestamp": 1585518814.8205473, "_step": 419}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": 3.5807535648345947, "Value Loss": 112.9704818725586, "_runtime": 9736.52898311615, "_timestamp": 1585518814.9497128, "_step": 420}
{"Episode reward": 91.80000000000001, "Episode length": 82, "Policy Loss": 2.5815491676330566, "Value Loss": 113.3149642944336, "_runtime": 9736.659000873566, "_timestamp": 1585518815.0797305, "_step": 421}
{"Episode reward": 91.70000000000002, "Episode length": 83, "Policy Loss": -1.5241096019744873, "Value Loss": 110.83699798583984, "_runtime": 9736.790739059448, "_timestamp": 1585518815.2114687, "_step": 422}
{"Episode reward": 91.60000000000002, "Episode length": 84, "Policy Loss": 0.5601031184196472, "Value Loss": 110.47258758544922, "_runtime": 9736.927912712097, "_timestamp": 1585518815.3486423, "_step": 423}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.6315290927886963, "Value Loss": 105.08672332763672, "_runtime": 9737.171854496002, "_timestamp": 1585518815.5925841, "_step": 424}
{"Episode reward": 83.50000000000004, "Episode length": 165, "Policy Loss": -8.33622932434082, "Value Loss": 58.94917297363281, "_runtime": 9738.666759729385, "_timestamp": 1585518817.0874894, "_step": 425}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 10.540189743041992, "Value Loss": 1.159605622291565, "_runtime": 9738.9254488945, "_timestamp": 1585518817.3461785, "_step": 426}
{"Episode reward": 83.70000000000005, "Episode length": 163, "Policy Loss": -1.4349943399429321, "Value Loss": 57.952213287353516, "_runtime": 9740.398519515991, "_timestamp": 1585518818.8192492, "_step": 427}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 10.436588287353516, "Value Loss": 1.1161246299743652, "_runtime": 9740.620110034943, "_timestamp": 1585518819.0408397, "_step": 428}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 3.122222661972046, "Value Loss": 100.44221496582031, "_runtime": 9740.77265381813, "_timestamp": 1585518819.1933835, "_step": 429}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": 0.22997017204761505, "Value Loss": 98.45262145996094, "_runtime": 9741.067359685898, "_timestamp": 1585518819.4880893, "_step": 430}
{"Episode reward": 83.00000000000003, "Episode length": 170, "Policy Loss": -4.111905097961426, "Value Loss": 55.85926055908203, "_runtime": 9742.56922864914, "_timestamp": 1585518820.9899583, "_step": 431}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 10.211143493652344, "Value Loss": 0.9577819108963013, "_runtime": 9742.85278081894, "_timestamp": 1585518821.2735105, "_step": 432}
{"Episode reward": 81.90000000000002, "Episode length": 181, "Policy Loss": -3.9799861907958984, "Value Loss": 52.66759490966797, "_runtime": 9744.329257249832, "_timestamp": 1585518822.749987, "_step": 433}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 10.075726509094238, "Value Loss": 0.9620927572250366, "_runtime": 9744.559294223785, "_timestamp": 1585518822.9800239, "_step": 434}
{"Episode reward": 90.10000000000002, "Episode length": 99, "Policy Loss": -0.3279136121273041, "Value Loss": 94.19649505615234, "_runtime": 9744.715881586075, "_timestamp": 1585518823.1366112, "_step": 435}
{"Episode reward": 90.30000000000003, "Episode length": 97, "Policy Loss": -0.5267208814620972, "Value Loss": 96.40677642822266, "_runtime": 9746.255094051361, "_timestamp": 1585518824.6758237, "_step": 436}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.944872856140137, "Value Loss": 0.8915677666664124, "_runtime": 9747.73991727829, "_timestamp": 1585518826.160647, "_step": 437}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.924659729003906, "Value Loss": 0.8943978548049927, "_runtime": 9747.899726390839, "_timestamp": 1585518826.320456, "_step": 438}
{"Episode reward": 90.80000000000003, "Episode length": 92, "Policy Loss": 1.1784034967422485, "Value Loss": 101.59925079345703, "_runtime": 9749.459211111069, "_timestamp": 1585518827.8799407, "_step": 439}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.89777660369873, "Value Loss": 0.990778923034668, "_runtime": 9750.994841814041, "_timestamp": 1585518829.4155715, "_step": 440}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.876616477966309, "Value Loss": 0.9415029287338257, "_runtime": 9751.162785291672, "_timestamp": 1585518829.583515, "_step": 441}
{"Episode reward": 90.30000000000003, "Episode length": 97, "Policy Loss": -1.0092154741287231, "Value Loss": 95.6527099609375, "_runtime": 9751.365854024887, "_timestamp": 1585518829.7865837, "_step": 442}
{"Episode reward": 90.9927379699747, "Episode length": 91, "Policy Loss": 0.9496628642082214, "Value Loss": 102.19850158691406, "_runtime": 9752.953510522842, "_timestamp": 1585518831.3742402, "_step": 443}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.894013404846191, "Value Loss": 0.9114516973495483, "_runtime": 9753.111968755722, "_timestamp": 1585518831.5326984, "_step": 444}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": 0.42217886447906494, "Value Loss": 102.75415802001953, "_runtime": 9753.360862731934, "_timestamp": 1585518831.7815924, "_step": 445}
{"Episode reward": 83.20000000000005, "Episode length": 168, "Policy Loss": -3.196707248687744, "Value Loss": 56.31266403198242, "_runtime": 9753.563995838165, "_timestamp": 1585518831.9847255, "_step": 446}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 4.29355525970459, "Value Loss": 100.80109405517578, "_runtime": 9753.708041667938, "_timestamp": 1585518832.1287713, "_step": 447}
{"Episode reward": 90.80000000000003, "Episode length": 92, "Policy Loss": 2.937884569168091, "Value Loss": 101.62901306152344, "_runtime": 9753.857550382614, "_timestamp": 1585518832.27828, "_step": 448}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 1.4278879165649414, "Value Loss": 99.40106964111328, "_runtime": 9754.002526760101, "_timestamp": 1585518832.4232564, "_step": 449}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 3.5885186195373535, "Value Loss": 100.95619201660156, "_runtime": 9754.142762899399, "_timestamp": 1585518832.5634925, "_step": 450}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": 0.45511022210121155, "Value Loss": 103.0648422241211, "_runtime": 9754.400193452835, "_timestamp": 1585518832.820923, "_step": 451}
{"Episode reward": 82.70000000000003, "Episode length": 173, "Policy Loss": -5.975627422332764, "Value Loss": 55.88601303100586, "_runtime": 9754.540711402893, "_timestamp": 1585518832.961441, "_step": 452}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": 1.1261332035064697, "Value Loss": 102.82177734375, "_runtime": 9756.03802394867, "_timestamp": 1585518834.4587536, "_step": 453}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.93268871307373, "Value Loss": 0.9771939516067505, "_runtime": 9756.207679748535, "_timestamp": 1585518834.6284094, "_step": 454}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": 1.6440036296844482, "Value Loss": 98.51991271972656, "_runtime": 9756.352895021439, "_timestamp": 1585518834.7736247, "_step": 455}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": -0.028711585327982903, "Value Loss": 100.0992431640625, "_runtime": 9756.55216383934, "_timestamp": 1585518834.9728935, "_step": 456}
{"Episode reward": 91.00000000000003, "Episode length": 90, "Policy Loss": 1.9141026735305786, "Value Loss": 103.36176300048828, "_runtime": 9756.688190221786, "_timestamp": 1585518835.1089199, "_step": 457}
{"Episode reward": 91.40000000000002, "Episode length": 86, "Policy Loss": -0.46810856461524963, "Value Loss": 107.11734771728516, "_runtime": 9756.838619470596, "_timestamp": 1585518835.259349, "_step": 458}
{"Episode reward": 90.30000000000003, "Episode length": 97, "Policy Loss": 0.6420512795448303, "Value Loss": 97.90177917480469, "_runtime": 9756.98799586296, "_timestamp": 1585518835.4087255, "_step": 459}
{"Episode reward": 90.20000000000002, "Episode length": 98, "Policy Loss": 5.1887006759643555, "Value Loss": 96.23950958251953, "_runtime": 9757.123725891113, "_timestamp": 1585518835.5444555, "_step": 460}
{"Episode reward": 91.30000000000003, "Episode length": 87, "Policy Loss": -3.778280735015869, "Value Loss": 105.90524291992188, "_runtime": 9757.268715143204, "_timestamp": 1585518835.6894448, "_step": 461}
{"Episode reward": 90.70000000000002, "Episode length": 93, "Policy Loss": 0.01814926229417324, "Value Loss": 100.40166473388672, "_runtime": 9757.412399291992, "_timestamp": 1585518835.833129, "_step": 462}
{"Episode reward": 90.80000000000003, "Episode length": 92, "Policy Loss": 0.772541880607605, "Value Loss": 101.55086517333984, "_runtime": 9758.908643484116, "_timestamp": 1585518837.3293731, "_step": 463}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.827058792114258, "Value Loss": 1.0550825595855713, "_runtime": 9760.391997814178, "_timestamp": 1585518838.8127275, "_step": 464}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.834698677062988, "Value Loss": 1.0438848733901978, "_runtime": 9760.550417661667, "_timestamp": 1585518838.9711473, "_step": 465}
{"Episode reward": 90.90000000000002, "Episode length": 91, "Policy Loss": 1.002600908279419, "Value Loss": 102.04692840576172, "_runtime": 9760.758501291275, "_timestamp": 1585518839.179231, "_step": 466}
{"Episode reward": 90.60000000000002, "Episode length": 94, "Policy Loss": -1.7245995998382568, "Value Loss": 98.61448669433594, "_runtime": 9760.938416719437, "_timestamp": 1585518839.3591464, "_step": 467}
{"Episode reward": 91.20000000000002, "Episode length": 88, "Policy Loss": -0.43148744106292725, "Value Loss": 105.24685668945312, "_runtime": 9761.178966522217, "_timestamp": 1585518839.5996962, "_step": 468}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": -3.262009620666504, "Value Loss": 58.66676330566406, "_runtime": 9761.421055078506, "_timestamp": 1585518839.8417847, "_step": 469}
{"Episode reward": 83.90000000000005, "Episode length": 161, "Policy Loss": -7.527957916259766, "Value Loss": 59.54426193237305, "_runtime": 9761.673779964447, "_timestamp": 1585518840.0945096, "_step": 470}
{"Episode reward": 83.10000000000004, "Episode length": 169, "Policy Loss": -6.403055191040039, "Value Loss": 57.78110122680664, "_runtime": 9761.823975086212, "_timestamp": 1585518840.2447047, "_step": 471}
{"Episode reward": 90.60000000000002, "Episode length": 94, "Policy Loss": -2.182180404663086, "Value Loss": 97.98575592041016, "_runtime": 9763.321526527405, "_timestamp": 1585518841.7422562, "_step": 472}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.779264450073242, "Value Loss": 0.9887588620185852, "_runtime": 9763.490248680115, "_timestamp": 1585518841.9109783, "_step": 473}
{"Episode reward": 90.50000000000003, "Episode length": 95, "Policy Loss": -0.16872690618038177, "Value Loss": 97.71041107177734, "_runtime": 9763.645145893097, "_timestamp": 1585518842.0658755, "_step": 474}
{"Episode reward": 90.00000000000003, "Episode length": 100, "Policy Loss": -0.45900294184684753, "Value Loss": 92.66732788085938, "_runtime": 9763.856056213379, "_timestamp": 1585518842.2767859, "_step": 475}
{"Episode reward": 90.20000000000002, "Episode length": 98, "Policy Loss": 1.5233755111694336, "Value Loss": 95.4292221069336, "_runtime": 9764.000106573105, "_timestamp": 1585518842.4208362, "_step": 476}
{"Episode reward": 90.80000000000003, "Episode length": 92, "Policy Loss": -0.6208816766738892, "Value Loss": 99.82811737060547, "_runtime": 9764.156686067581, "_timestamp": 1585518842.5774157, "_step": 477}
{"Episode reward": 89.90000000000002, "Episode length": 101, "Policy Loss": 0.2561532258987427, "Value Loss": 92.1494369506836, "_runtime": 9765.652174949646, "_timestamp": 1585518844.0729046, "_step": 478}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.749228477478027, "Value Loss": 1.0150271654129028, "_runtime": 9767.137355089188, "_timestamp": 1585518845.5580847, "_step": 479}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.763251304626465, "Value Loss": 1.0065217018127441, "_runtime": 9768.64185142517, "_timestamp": 1585518847.062581, "_step": 480}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.75937271118164, "Value Loss": 1.0099244117736816, "_runtime": 9770.218586921692, "_timestamp": 1585518848.6393166, "_step": 481}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.73286247253418, "Value Loss": 1.108274221420288, "_runtime": 9771.752629995346, "_timestamp": 1585518850.1733596, "_step": 482}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.690945625305176, "Value Loss": 0.9697059988975525, "_runtime": 9773.293144464493, "_timestamp": 1585518851.713874, "_step": 483}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.638419151306152, "Value Loss": 1.1839704513549805, "_runtime": 9774.850723743439, "_timestamp": 1585518853.2714534, "_step": 484}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.524840354919434, "Value Loss": 0.9291121363639832, "_runtime": 9776.410059690475, "_timestamp": 1585518854.8307893, "_step": 485}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.485156059265137, "Value Loss": 1.1239866018295288, "_runtime": 9777.951976060867, "_timestamp": 1585518856.3727057, "_step": 486}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.373438835144043, "Value Loss": 0.9530695676803589, "_runtime": 9779.566488981247, "_timestamp": 1585518857.9872186, "_step": 487}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.283162117004395, "Value Loss": 1.1869597434997559, "_runtime": 9781.135129213333, "_timestamp": 1585518859.5558589, "_step": 488}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.133056640625, "Value Loss": 0.8103771209716797, "_runtime": 9782.685425281525, "_timestamp": 1585518861.106155, "_step": 489}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 9.04406452178955, "Value Loss": 0.7487017512321472, "_runtime": 9784.265942573547, "_timestamp": 1585518862.6866722, "_step": 490}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.906079292297363, "Value Loss": 0.7578227519989014, "_runtime": 9785.839372873306, "_timestamp": 1585518864.2601025, "_step": 491}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.783956527709961, "Value Loss": 1.0480858087539673, "_runtime": 9787.402089595795, "_timestamp": 1585518865.8228192, "_step": 492}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.660728454589844, "Value Loss": 0.6725270748138428, "_runtime": 9788.980092525482, "_timestamp": 1585518867.4008222, "_step": 493}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.531078338623047, "Value Loss": 0.6894864439964294, "_runtime": 9790.547718048096, "_timestamp": 1585518868.9684477, "_step": 494}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.392337799072266, "Value Loss": 0.6394436955451965, "_runtime": 9792.111385822296, "_timestamp": 1585518870.5321155, "_step": 495}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.263288497924805, "Value Loss": 0.6409626603126526, "_runtime": 9793.691009283066, "_timestamp": 1585518872.111739, "_step": 496}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 8.11424732208252, "Value Loss": 0.603330135345459, "_runtime": 9795.266111850739, "_timestamp": 1585518873.6868415, "_step": 497}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 7.973853588104248, "Value Loss": 0.6082586646080017, "gradients/fc.2.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "gradients/fc.2.weight": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "gradients/fc.1.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "gradients/fc.1.weight": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 512.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "gradients/fc.0.bias": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "gradients/fc.0.weight": {"_type": "histogram", "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 64.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "bins": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "_runtime": 9796.834160804749, "_timestamp": 1585518875.2548904, "_step": 498}
{"Episode reward": -99.8999999999986, "Episode length": 999, "Policy Loss": 7.861328601837158, "Value Loss": 0.609018862247467, "_runtime": 9796.834160804749, "_timestamp": 1585518875.2548904, "_step": 499}
