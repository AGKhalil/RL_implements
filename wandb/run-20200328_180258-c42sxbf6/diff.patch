diff --git a/main_ac_mcc.py b/main_ac_mcc.py
index b51e4ad..e0f666a 100644
--- a/main_ac_mcc.py
+++ b/main_ac_mcc.py
@@ -22,7 +22,7 @@ from policies import MLP_AC
 import argparse
 
 
-def visualize():
+def visualize(env, ac, cr):
     done = False
     obs = env.reset()
     imgs, visited_pos, visited_vel, acts, means, stds, vals = [], [], [], [], [], [], []
@@ -45,7 +45,22 @@ def visualize():
 
     return visited_pos, visited_vel, acts, means, stds, vals
 
-def net_layers(hidden):
+def evaluate(env, ac, cr):
+	eval_rew = []
+	for _ in range(100):
+	    done = False
+	    obs = env.reset()
+	    ep_reward = 0
+	    while not done:
+	        act, _ = ac.get_action(obs)
+	        acts.append(act[0])
+	        obs, rew, done, _ = env.step(act)
+	        ep_reward += rew
+	    eval_rew.append(ep_reward)
+
+	return np.mean(eval_rew)
+
+def net_layers(hidden, env_type, env):
     if env_type == 'DISCRETE':
         act_space = env.action_space.n
     else:
@@ -54,164 +69,170 @@ def net_layers(hidden):
     return [obs_space] + hidden + [act_space]
 
 
-wandb.init(entity="agkhalil", project="pytorch-ac-mountaincarcont")
-wandb.watch_called = False
-
-parser = argparse.ArgumentParser(description='PyTorch actor-critic example')
-parser.add_argument('--lr_ac', type=float, default=0.001, metavar='lrac', help='actor learning rate')
-parser.add_argument('--lr_cr', type=float, default=0.000001, metavar='lrac',help='critic learning rate')
-args = parser.parse_args()
-
-config = wandb.config
-config.batch_size = 50
-config.episodes = 10000
-config.lr_ac = args.lr_ac
-config.lr_cr = args.lr_cr
-config.seed = 42
-config.gamma = 0.99
-eps = np.finfo(np.float32).eps.item()
-
-device = torch.device('cpu')
-torch.manual_seed(config.seed)
-lr_ac = config.lr_ac
-lr_cr = config.lr_cr
-batch_size = config.batch_size
-
-env = gym.make('MountainCarContinuous-v0')
-env_type = 'CONT'
-
-mlp_ac = MLP_AC(net_layers([32, 16])).to(device)
-mlp_cr = MLP_AC(net_layers([64, 32])).to(device)
-ac = AC(mlp_ac, env, device, env_type)
-cr = AC(mlp_cr, env, device, env_type)
-optimizer_cr = optim.Adam(cr.policy.parameters(), lr=lr_cr)
-optimizer_ac = optim.Adam(ac.policy.parameters(), lr=lr_ac)
-loss_fn = torch.nn.MSELoss()
-
-EPISODES = config.episodes
-gamma = config.gamma
-
-wandb.watch(ac.policy, log="all")
-
-visited_pos, visited_vel = [], []
-
-for episode in tqdm(range(0, EPISODES)):
-    rewards = []
-    log_probs = []
-    values_list = []
-    next_values_list = []
-    obs = env.reset()
-    done = False
-    ep_reward = 0
-    step = 0
-    while not done:
-        action, log_prob = ac.get_action(obs)
-        value = cr.get_action(obs, critic=True)
-        new_obs, rew, done, _ = env.step(action)
-        next_value = cr.get_action(new_obs, critic=True)
-        ep_reward += rew
-        rewards.append(rew)
-        log_probs.append(log_prob)
-        values_list.append(value)
-        next_values_list.append(next_value)
-        step += 1
-        obs = new_obs
-
-    rewards_size = len(rewards)
-    gammas = [np.power(gamma, i) for i in range(rewards_size)]
-    discounted_rewards = [np.sum(np.multiply(gammas[:rewards_size-i], rewards[i:])) for i in range(rewards_size)]
-    discounted_rewards = torch.tensor(discounted_rewards).to(device)
-    returns = [rewards[i] + gamma * next_values_list[i] for i in reversed(range(rewards_size))]
-    # returns = torch.tensor(returns).to(device)
-    # values_list = torch.tensor(values_list).to(device)
-    # next_values_list = torch.tensor(next_values_list).to(device)
-
-    # target = rew + gamma * next_value.detach()
-    td = np.subtract(returns, values_list)
-    values_list = torch.stack(values_list)
-    returns = torch.stack(returns)
-    loss_cr = loss_fn(values_list, returns)
-    loss_ac = [-td[i].detach() * log_probs[i] for i in range(len(td))]
-    loss_ac = torch.stack(loss_ac)
-    # loss_ac.to(device)
-    # loss_cr.to(device)
-    
-    optimizer_ac.zero_grad()
-    optimizer_cr.zero_grad()
-
-    # loss = loss_cr + loss_ac
-    # print(action, loss_ac)
-    
-    loss_ac.sum().backward()
-    loss_cr.backward()
-    optimizer_cr.step()
-    optimizer_ac.step()
-
-
-    # for i in list(ac.policy.fc[0].parameters()):
-    #     weird_sum += i.detach().sum().item()
-    # print(weird_sum)
-
-    # rewards_size = len(rewards)
-    # gammas = [np.power(gamma, i) for i in range(rewards_size)]
-    # discounted_rewards = [np.sum(np.multiply(gammas[:rewards_size-i], rewards[i:])) for i in range(rewards_size)]
-    # optimizer.zero_grad()
-    # discounted_rewards = torch.tensor(discounted_rewards).to(device)
-    # advantage = discounted_rewards #- discounted_rewards.mean()) / (discounted_rewards.std() + eps)
-    # loss = [-advantage[i] * log_probs[i] for i in range(len(advantage))]
-    # loss = torch.stack(loss)
-    # loss.to(device)
-    # loss.sum().backward()
-    # optimizer.step()
-    wandb.log({
-        "Episode reward": ep_reward,
-        "Episode length": step,
-        "Policy Loss": loss_ac.cpu().mean(),
-        "Value Loss": loss_cr.cpu().mean(),
-        }, step=episode)
-
-    if episode % 500 == 0 and episode != 0:
-        visited_pos, visited_vel, acts, means, stds, vals = visualize()
-        fig1 = plt.figure()
-        plt.scatter(visited_pos, visited_vel, marker='.')
-        plt.xlabel('position')
-        plt.ylabel('velocity')
-        
-        fig2 = plt.figure()
-        plt.scatter([i for i in range(len(acts))], acts, marker='.')
-        plt.xlabel('steps')
-        plt.ylabel('actions')
-        
-        fig3 = plt.figure()
-        plt.scatter([i for i in range(len(means))], means, marker='.')
-        plt.xlabel('steps')
-        plt.ylabel('means')
-
-        fig4 = plt.figure()
-        plt.scatter([i for i in range(len(vals))], vals, marker='.')
-        plt.xlabel('steps')
-        plt.ylabel('values')
-
-        fig5 = plt.figure()
-        plt.scatter([i for i in range(len(stds))], stds, marker='.')
-        plt.xlabel('steps')
-        plt.ylabel('stds')
-        
-        wandb.log({
-     #       "video": wandb.Video('/tmp/current_gif.gif', fps=4, format="gif"),
-            "visited_pos": visited_pos,
-            "visited_vel": visited_vel,
-            "actions": acts,
-            "means": means,
-            "values": vals,
-            "states": fig1,
-            "actions/step": fig2,
-            "means/step": fig3,
-            "values/step": fig4,
-            "stds/step": fig5,
-            })
-       # model_name = "model-" + str(episode) + ".h5"
-       # torch.save(ac.policy.state_dict(), model_name)
-       # wandb.save(model_name)
-       # dir_path = os.path.dirname(os.path.realpath(__file__))
-       # os.remove(dir_path + '/' + model_name)
+def main():
+	wandb.init(entity="agkhalil", project="pytorch-ac-mountaincarcont")
+	wandb.watch_called = False
+
+	parser = argparse.ArgumentParser(description='PyTorch actor-critic example')
+	parser.add_argument('--lr_ac', type=float, default=0.001, metavar='lrac', help='actor learning rate')
+	parser.add_argument('--lr_cr', type=float, default=0.000001, metavar='lrac',help='critic learning rate')
+	args = parser.parse_args()
+
+	config = wandb.config
+	config.batch_size = 50
+	config.episodes = 1000
+	config.lr_ac = args.lr_ac
+	config.lr_cr = args.lr_cr
+	config.seed = 42
+	config.gamma = 0.99
+	eps = np.finfo(np.float32).eps.item()
+
+	device = torch.device('cpu')
+	torch.manual_seed(config.seed)
+	lr_ac = config.lr_ac
+	lr_cr = config.lr_cr
+	batch_size = config.batch_size
+
+	env = gym.make('MountainCarContinuous-v0')
+	env_type = 'CONT'
+
+	mlp_ac = MLP_AC(net_layers([32, 16], env_type, env)).to(device)
+	mlp_cr = MLP_AC(net_layers([64, 32], env_type, env)).to(device)
+	ac = AC(mlp_ac, env, device, env_type)
+	cr = AC(mlp_cr, env, device, env_type)
+	optimizer_cr = optim.Adam(cr.policy.parameters(), lr=lr_cr)
+	optimizer_ac = optim.Adam(ac.policy.parameters(), lr=lr_ac)
+	loss_fn = torch.nn.MSELoss()
+
+	EPISODES = config.episodes
+	gamma = config.gamma
+
+	wandb.watch(ac.policy, log="all")
+
+	visited_pos, visited_vel = [], []
+
+	for episode in tqdm(range(0, EPISODES)):
+	    rewards = []
+	    log_probs = []
+	    values_list = []
+	    next_values_list = []
+	    obs = env.reset()
+	    done = False
+	    ep_reward = 0
+	    step = 0
+	    while not done:
+	        action, log_prob = ac.get_action(obs)
+	        value = cr.get_action(obs, critic=True)
+	        new_obs, rew, done, _ = env.step(action)
+	        next_value = cr.get_action(new_obs, critic=True)
+	        ep_reward += rew
+	        rewards.append(rew)
+	        log_probs.append(log_prob)
+	        values_list.append(value)
+	        next_values_list.append(next_value)
+	        step += 1
+	        obs = new_obs
+
+	    rewards_size = len(rewards)
+	    gammas = [np.power(gamma, i) for i in range(rewards_size)]
+	    discounted_rewards = [np.sum(np.multiply(gammas[:rewards_size-i], rewards[i:])) for i in range(rewards_size)]
+	    discounted_rewards = torch.tensor(discounted_rewards).to(device)
+	    returns = [rewards[i] + gamma * next_values_list[i] for i in reversed(range(rewards_size))]
+	    # returns = torch.tensor(returns).to(device)
+	    # values_list = torch.tensor(values_list).to(device)
+	    # next_values_list = torch.tensor(next_values_list).to(device)
+
+	    # target = rew + gamma * next_value.detach()
+	    td = np.subtract(returns, values_list)
+	    values_list = torch.stack(values_list)
+	    returns = torch.stack(returns)
+	    loss_cr = loss_fn(values_list, returns)
+	    loss_ac = [-td[i].detach() * log_probs[i] for i in range(len(td))]
+	    loss_ac = torch.stack(loss_ac)
+	    # loss_ac.to(device)
+	    # loss_cr.to(device)
+	    
+	    optimizer_ac.zero_grad()
+	    optimizer_cr.zero_grad()
+
+	    # loss = loss_cr + loss_ac
+	    # print(action, loss_ac)
+	    
+	    loss_ac.sum().backward()
+	    loss_cr.backward()
+	    optimizer_cr.step()
+	    optimizer_ac.step()
+
+
+	    # for i in list(ac.policy.fc[0].parameters()):
+	    #     weird_sum += i.detach().sum().item()
+	    # print(weird_sum)
+
+	    # rewards_size = len(rewards)
+	    # gammas = [np.power(gamma, i) for i in range(rewards_size)]
+	    # discounted_rewards = [np.sum(np.multiply(gammas[:rewards_size-i], rewards[i:])) for i in range(rewards_size)]
+	    # optimizer.zero_grad()
+	    # discounted_rewards = torch.tensor(discounted_rewards).to(device)
+	    # advantage = discounted_rewards #- discounted_rewards.mean()) / (discounted_rewards.std() + eps)
+	    # loss = [-advantage[i] * log_probs[i] for i in range(len(advantage))]
+	    # loss = torch.stack(loss)
+	    # loss.to(device)
+	    # loss.sum().backward()
+	    # optimizer.step()
+	    wandb.log({
+	        "Episode reward": ep_reward,
+	        "Episode length": step,
+	        "Policy Loss": loss_ac.cpu().mean(),
+	        "Value Loss": loss_cr.cpu().mean(),
+	        }, step=episode)
+
+	    if episode % 500 == 0 and episode != 0:
+	        visited_pos, visited_vel, acts, means, stds, vals = visualize(env, ac, cr)
+	        fig1 = plt.figure()
+	        plt.scatter(visited_pos, visited_vel, marker='.')
+	        plt.xlabel('position')
+	        plt.ylabel('velocity')
+	        
+	        fig2 = plt.figure()
+	        plt.scatter([i for i in range(len(acts))], acts, marker='.')
+	        plt.xlabel('steps')
+	        plt.ylabel('actions')
+	        
+	        fig3 = plt.figure()
+	        plt.scatter([i for i in range(len(means))], means, marker='.')
+	        plt.xlabel('steps')
+	        plt.ylabel('means')
+
+	        fig4 = plt.figure()
+	        plt.scatter([i for i in range(len(vals))], vals, marker='.')
+	        plt.xlabel('steps')
+	        plt.ylabel('values')
+
+	        fig5 = plt.figure()
+	        plt.scatter([i for i in range(len(stds))], stds, marker='.')
+	        plt.xlabel('steps')
+	        plt.ylabel('stds')
+	        
+	        wandb.log({
+	     #       "video": wandb.Video('/tmp/current_gif.gif', fps=4, format="gif"),
+	            "visited_pos": visited_pos,
+	            "visited_vel": visited_vel,
+	            "actions": acts,
+	            "means": means,
+	            "values": vals,
+	            "states": fig1,
+	            "actions/step": fig2,
+	            "means/step": fig3,
+	            "values/step": fig4,
+	            "stds/step": fig5,
+	            })
+	       # model_name = "model-" + str(episode) + ".h5"
+	       # torch.save(ac.policy.state_dict(), model_name)
+	       # wandb.save(model_name)
+	       # dir_path = os.path.dirname(os.path.realpath(__file__))
+	       # os.remove(dir_path + '/' + model_name)
+
+	return evaluate(env, ac, cr)
+
+if __name__ == "__main__":
+	main()
diff --git a/wandb/debug.log b/wandb/debug.log
index 51a7837..4c2c4e0 100644
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1,20 +1,18 @@
-2020-03-27 14:10:47,410 DEBUG   MainThread:53 [wandb_config.py:_load_defaults():122] no defaults not found in config-defaults.yaml
-2020-03-27 14:10:47,415 DEBUG   MainThread:53 [meta.py:setup():97] code probe starting
-2020-03-27 14:10:47,416 DEBUG   MainThread:53 [meta.py:_setup_code_git():49] probe for git information
-2020-03-27 14:10:47,423 DEBUG   MainThread:53 [meta.py:_setup_code_program():58] save program starting
-2020-03-27 14:10:47,423 DEBUG   MainThread:53 [meta.py:_setup_code_program():60] save program starting: /app/./dqn.py
-2020-03-27 14:10:47,423 DEBUG   MainThread:53 [meta.py:_setup_code_program():65] save program saved: /app/wandb/run-20200327_141041-2y39lveo/code/dqn.py
-2020-03-27 14:10:47,423 DEBUG   MainThread:53 [meta.py:_setup_code_program():67] save program
-2020-03-27 14:10:47,423 DEBUG   MainThread:53 [meta.py:setup():119] code probe done
-2020-03-27 14:10:47,432 DEBUG   MainThread:53 [run_manager.py:__init__():541] Initialized sync for pytorch-vpg-mountaincar/2y39lveo
-2020-03-27 14:10:47,434 INFO    MainThread:53 [run_manager.py:wrap_existing_process():1144] wrapping existing process 25
-2020-03-27 14:10:47,434 WARNING MainThread:53 [io_wrap.py:register():104] SIGWINCH handler was not None: <Handlers.SIG_DFL: 0>
-2020-03-27 14:10:47,474 INFO    MainThread:53 [run_manager.py:init_run():924] system metrics and metadata threads started
-2020-03-27 14:10:47,474 INFO    MainThread:53 [run_manager.py:wrap_existing_process():1161] informing user process we are ready to proceed
-2020-03-27 14:10:47,474 INFO    MainThread:53 [run_manager.py:_sync_etc():1268] entering loop for messages from user process
-2020-03-27 14:10:48,414 INFO    Thread-3  :53 [run_manager.py:_on_file_modified():688] file/dir modified: /app/wandb/run-20200327_141041-2y39lveo/wandb-history.jsonl
-2020-03-27 14:10:51,479 INFO    MainThread:53 [run_manager.py:_sync_etc():1291] received message from user process: {"exitcode": 1}
-2020-03-27 14:10:51,479 INFO    MainThread:53 [run_manager.py:_sync_etc():1377] closing log streams and sending exitcode to W&B
-2020-03-27 14:10:51,479 INFO    MainThread:53 [run_manager.py:shutdown():1068] shutting down system stats and metadata service
-_sync_etc():1377] closing log streams and sending exitcode to W&B
-2020-03-27 14:10:51,479 INFO    MainThread:53 [2y39lveo:run_manager.py:shutdown():1068] shutting down system stats and metadata service
+2020-03-28 18:02:59,107 DEBUG   MainThread:3103 [wandb_config.py:_load_defaults():111] no defaults not found in config-defaults.yaml
+2020-03-28 18:02:59,117 DEBUG   MainThread:3103 [cmd.py:execute():728] Popen(['git', 'cat-file', '--batch-check'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=<valid stream>)
+2020-03-28 18:02:59,133 DEBUG   MainThread:3103 [cmd.py:execute():728] Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
+2020-03-28 18:02:59,143 DEBUG   MainThread:3103 [cmd.py:execute():728] Popen(['git', 'status', '--porcelain', '--untracked-files'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
+2020-03-28 18:02:59,178 DEBUG   MainThread:3103 [run_manager.py:__init__():535] Initialized sync for pytorch-ac-mountaincarcont/c42sxbf6
+2020-03-28 18:02:59,190 INFO    MainThread:3103 [run_manager.py:wrap_existing_process():1133] wrapping existing process 3098
+2020-03-28 18:02:59,190 WARNING MainThread:3103 [io_wrap.py:register():104] SIGWINCH handler was not None: <Handlers.SIG_DFL: 0>
+2020-03-28 18:02:59,199 DEBUG   MainThread:3103 [connectionpool.py:_new_conn():959] Starting new HTTPS connection (1): pypi.org:443
+2020-03-28 18:02:59,290 DEBUG   MainThread:3103 [connectionpool.py:_make_request():437] https://pypi.org:443 "GET /pypi/wandb/json HTTP/1.1" 200 42295
+2020-03-28 18:02:59,313 INFO    MainThread:3103 [run_manager.py:init_run():918] system metrics and metadata threads started
+2020-03-28 18:02:59,313 INFO    MainThread:3103 [run_manager.py:init_run():952] upserting run before process can begin, waiting at most 10 seconds
+2020-03-28 18:02:59,321 DEBUG   Thread-14 :3103 [connectionpool.py:_new_conn():959] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-03-28 18:02:59,625 DEBUG   Thread-14 :3103 [connectionpool.py:_make_request():437] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-03-28 18:02:59,631 INFO    Thread-14 :3103 [run_manager.py:_upsert_run():1037] saving patches
+2020-03-28 18:02:59,631 DEBUG   Thread-14 :3103 [cmd.py:execute():728] Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
+2020-03-28 18:02:59,644 DEBUG   Thread-14 :3103 [cmd.py:execute():728] Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
+2020-03-28 18:02:59,657 DEBUG   Thread-14 :3103 [cmd.py:execute():728] Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
+2020-03-28 18:02:59,676 DEBUG   Thread-14 :3103 [cmd.py:execute():728] Popen(['git', 'version'], cwd=/Users/khela/code_bases/RL_implements, universal_newlines=False, shell=None, istream=None)
